{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-fjEiMF7hY-"
      },
      "outputs": [],
      "source": [
        "! pip install gradio\n",
        "! git clone \"https://github.com/mohsenfayyaz/DecompX\"\n",
        "! pip install datasets==1.18.3\n",
        "! pip install transformers==4.18.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "from IPython.display import display, HTML\n",
        "from transformers import AutoTokenizer\n",
        "from DecompX.src.decompx_utils import DecompXConfig\n",
        "from DecompX.src.modeling_bert import BertForSequenceClassification\n",
        "from DecompX.src.modeling_roberta import RobertaForSequenceClassification\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "MODELS = ['TehranNLP-org/bert-base-uncased-cls-sst2', 'TehranNLP-org/bert-large-sst2', \"WillHeld/roberta-base-sst2\"]\n",
        "\n",
        "def plot_clf(tokens, logits, label_names, title=\"\", file_name=None):\n",
        "    print(tokens)\n",
        "    plt.figure(figsize=(4.5, 5))\n",
        "    colors = [\"#019875\" if l else \"#B8293D\" for l in (logits >= 0)]\n",
        "    plt.barh(range(len(tokens)), logits, color=colors)\n",
        "    plt.axvline(0, color='black', ls='-', lw=2, alpha=0.2)\n",
        "    plt.gca().invert_yaxis()\n",
        "    \n",
        "    max_limit = np.max(np.abs(logits)) + 0.2\n",
        "    min_limit = -0.01 if np.min(logits) > 0 else -max_limit\n",
        "    plt.xlim(min_limit, max_limit)\n",
        "    plt.gca().set_xticks([min_limit, max_limit])\n",
        "    plt.gca().set_xticklabels(label_names, fontsize=14, fontweight=\"bold\")\n",
        "    plt.gca().set_yticks(range(len(tokens)))\n",
        "    plt.gca().set_yticklabels(tokens)\n",
        "\n",
        "    plt.gca().yaxis.tick_right()\n",
        "    for xtick, color in zip(plt.gca().get_yticklabels(), colors):\n",
        "        xtick.set_color(color)\n",
        "        xtick.set_fontweight(\"bold\")\n",
        "        xtick.set_verticalalignment(\"center\")\n",
        "\n",
        "    for xtick, color in zip(plt.gca().get_xticklabels(), [\"#B8293D\", \"#019875\"]):\n",
        "        xtick.set_color(color)\n",
        "    # plt.title(title, fontsize=14, fontweight=\"bold\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "\n",
        "def print_importance(importance, tokenized_text, discrete=False, prefix=\"\", no_cls_sep=False):\n",
        "    \"\"\"\n",
        "    importance: (sent_len)\n",
        "    \"\"\"\n",
        "    if no_cls_sep:\n",
        "        importance = importance[1:-1]\n",
        "        tokenized_text = tokenized_text[1:-1]\n",
        "    importance = importance / np.abs(importance).max() / 1.5  # Normalize\n",
        "    if discrete:\n",
        "        importance = np.argsort(np.argsort(importance)) / len(importance) / 1.6\n",
        "    \n",
        "    html = \"<pre style='color:black; padding: 3px;'>\"+prefix\n",
        "    for i in range(len(tokenized_text)):\n",
        "        if importance[i] >= 0:\n",
        "            rgba = matplotlib.colormaps.get_cmap('Greens')(importance[i])   # Wistia\n",
        "        else:\n",
        "            rgba = matplotlib.colormaps.get_cmap('Reds')(np.abs(importance[i]))   # Wistia\n",
        "        text_color = \"color: rgba(255, 255, 255, 1.0); \" if np.abs(importance[i]) > 0.9 else \"\"\n",
        "        color = f\"background-color: rgba({rgba[0]*255}, {rgba[1]*255}, {rgba[2]*255}, {rgba[3]}); \" + text_color\n",
        "        html += (f\"<span style='\"\n",
        "                 f\"{color}\"\n",
        "                 f\"color:black; border-radius: 5px; padding: 3px;\"\n",
        "                 f\"font-weight: {int(800)};\"\n",
        "                 \"'>\")\n",
        "        html += tokenized_text[i].replace('<', \"[\").replace(\">\", \"]\")\n",
        "        html += \"</span> \"\n",
        "    html += \"</pre>\"\n",
        "    # display(HTML(html))\n",
        "    return html\n",
        "\n",
        "def print_preview(decompx_outputs_df, idx=0, discrete=False):\n",
        "    html = \"\"\n",
        "    NO_CLS_SEP = False\n",
        "    df = decompx_outputs_df\n",
        "    for col in [\"importance_last_layer_aggregated\", \"importance_last_layer_classifier\"]:\n",
        "        if col in df and df[col][idx] is not None:\n",
        "            if \"aggregated\" in col:\n",
        "                sentence_importance = df[col].iloc[idx][0, :]\n",
        "            if \"classifier\" in col:\n",
        "                for label in range(df[col].iloc[idx].shape[-1]):\n",
        "                    sentence_importance = df[col].iloc[idx][:, label]\n",
        "                    html += print_importance(\n",
        "                        sentence_importance,\n",
        "                        df[\"tokens\"].iloc[idx], \n",
        "                        prefix=f\"{col.split('_')[-1]} Label{label}:\".ljust(20),\n",
        "                        no_cls_sep=NO_CLS_SEP,\n",
        "                        discrete=False\n",
        "                    )\n",
        "                break\n",
        "                sentence_importance = df[col].iloc[idx][:, df[\"label\"].iloc[idx]]\n",
        "            html += print_importance(\n",
        "                sentence_importance,\n",
        "                df[\"tokens\"].iloc[idx], \n",
        "                prefix=f\"{col.split('_')[-1]}:\".ljust(20),\n",
        "                no_cls_sep=NO_CLS_SEP,\n",
        "                discrete=discrete\n",
        "            )\n",
        "    return \"<div style='overflow:auto; background-color:white; padding: 10px;'>\" + html\n",
        "\n",
        "def run_decompx(text, model):\n",
        "    \"\"\"\n",
        "    Provide DecompX Token Explanation of Model on Text\n",
        "    \"\"\"\n",
        "    SENTENCES = [text, \"nothing\"]\n",
        "    CONFIGS = {\n",
        "        \"DecompX\":\n",
        "            DecompXConfig(\n",
        "                include_biases=True,\n",
        "                bias_decomp_type=\"absdot\",\n",
        "                include_LN1=True,\n",
        "                include_FFN=True,\n",
        "                FFN_approx_type=\"GeLU_ZO\",\n",
        "                include_LN2=True,\n",
        "                aggregation=\"vector\",\n",
        "                include_classifier_w_pooler=True,\n",
        "                tanh_approx_type=\"ZO\",\n",
        "                output_all_layers=True,\n",
        "                output_attention=None,\n",
        "                output_res1=None,\n",
        "                output_LN1=None,\n",
        "                output_FFN=None,\n",
        "                output_res2=None,\n",
        "                output_encoder=None,\n",
        "                output_aggregated=\"norm\",\n",
        "                output_pooler=\"norm\",\n",
        "                output_classifier=True,\n",
        "            ),\n",
        "    }\n",
        "    MODEL = model\n",
        "    # LOAD MODEL AND TOKENIZER\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "    tokenized_sentence = tokenizer(SENTENCES, return_tensors=\"pt\", padding=True)\n",
        "    batch_lengths = tokenized_sentence['attention_mask'].sum(dim=-1)\n",
        "    if \"roberta\" in MODEL:\n",
        "        model = RobertaForSequenceClassification.from_pretrained(MODEL)\n",
        "    elif \"bert\" in MODEL:\n",
        "        model = BertForSequenceClassification.from_pretrained(MODEL)\n",
        "    else:\n",
        "        raise Exception(f\"Not implented model: {MODEL}\")\n",
        "    # RUN DECOMPX\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        logits, hidden_states, decompx_last_layer_outputs, decompx_all_layers_outputs = model(\n",
        "            **tokenized_sentence, \n",
        "            output_attentions=False, \n",
        "            return_dict=False, \n",
        "            output_hidden_states=True, \n",
        "            decompx_config=CONFIGS[\"DecompX\"]\n",
        "        )\n",
        "    decompx_outputs = {\n",
        "        \"tokens\": [tokenizer.convert_ids_to_tokens(tokenized_sentence[\"input_ids\"][i][:batch_lengths[i]]) for i in range(len(SENTENCES))],\n",
        "        \"logits\": logits.cpu().detach().numpy().tolist(),  # (batch, classes)\n",
        "        \"cls\": hidden_states[-1][:, 0, :].cpu().detach().numpy().tolist()# Last layer & only CLS -> (batch, emb_dim)\n",
        "    }\n",
        "\n",
        "    ### decompx_last_layer_outputs.classifier ~ (8, 55, 2) ###\n",
        "    importance = np.array([g.squeeze().cpu().detach().numpy() for g in decompx_last_layer_outputs.classifier]).squeeze()  # (batch, seq_len, classes)\n",
        "    importance = [importance[j][:batch_lengths[j], :] for j in range(len(importance))]\n",
        "    decompx_outputs[\"importance_last_layer_classifier\"] = importance\n",
        "\n",
        "    ### decompx_all_layers_outputs.aggregated ~ (12, 8, 55, 55) ###\n",
        "    importance = np.array([g.squeeze().cpu().detach().numpy() for g in decompx_all_layers_outputs.aggregated])  # (layers, batch, seq_len, seq_len)\n",
        "    importance = np.einsum('lbij->blij', importance)  # (batch, layers, seq_len, seq_len)\n",
        "    importance = [importance[j][:, :batch_lengths[j], :batch_lengths[j]] for j in range(len(importance))]\n",
        "    decompx_outputs[\"importance_all_layers_aggregated\"] = importance\n",
        "\n",
        "    decompx_outputs_df = pd.DataFrame(decompx_outputs)\n",
        "    idx = 0\n",
        "    pred_label = np.argmax(decompx_outputs_df.iloc[idx][\"logits\"], axis=-1)\n",
        "    label = decompx_outputs_df.iloc[idx][\"importance_last_layer_classifier\"][:, pred_label]\n",
        "    tokens = decompx_outputs_df.iloc[idx][\"tokens\"][1:-1]\n",
        "    label = label[1:-1]\n",
        "    label = label / np.max(np.abs(label))\n",
        "    plot_clf(tokens, label, ['-','+'], title=f\"DecompX for Predicted Label: {pred_label}\", file_name=\"example_sst2_our_method\")\n",
        "    return plt, print_preview(decompx_outputs_df)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=run_decompx,\n",
        "    inputs=[\n",
        "        gr.components.Textbox(label=\"Text\"),\n",
        "        gr.components.Dropdown(label=\"Model\", choices=MODELS),\n",
        "    ],\n",
        "    outputs=[\"plot\", \"html\"],\n",
        "    examples=[\n",
        "        [\"a good piece of work more often than not.\", \"TehranNLP-org/bert-base-uncased-cls-sst2\"], \n",
        "        [\"a good piece of work more often than not.\", \"TehranNLP-org/bert-large-sst2\"], \n",
        "        [\"a good piece of work more often than not.\", \"WillHeld/roberta-base-sst2\"], \n",
        "        [\"A deep and meaningful film.\", \"TehranNLP-org/bert-base-uncased-cls-sst2\"],\n",
        "    ],\n",
        "    cache_examples=True,\n",
        "    title=\"DecompX Demo\",\n",
        "    description=\"This is a demo for the ACL 2023 paper [DecompX](https://github.com/mohsenfayyaz/DecompX/)\"\n",
        ")\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "7_7fmA5T8KX0",
        "outputId": "57b3642a-e085-46a7-e5e2-f94a35217965"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CE03VNJtadmx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}