{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import pickle\n",
    "import pathlib\n",
    "import os\n",
    "from importlib import reload\n",
    "import sys\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers.trainer_pt_utils import LengthGroupedSampler\n",
    "\n",
    "# For Imports\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "print(\"sys.path:\", sys.path)\n",
    "\n",
    "from GlobEnc.src.modeling.globenc_utils import GlobencConfig\n",
    "from GlobEnc.src.modeling.modeling_bert_v3 import BertForSequenceClassification\n",
    "from GlobEnc.src.modeling.modeling_roberta import RobertaForSequenceClassification\n",
    "\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97242498",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f5a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"/home/modaresi/projects/globenc_analysis/outputs/globencs_v3\"\n",
    "MODELS_DIR = \"/home/modaresi/projects/globenc_analysis/outputs/models\"\n",
    "\n",
    "MODEL_DATASET_SET = [\n",
    "#     (f\"{MODELS_DIR}/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-10525\", \"sst2\", \"validation\"),\n",
    "#     (f\"{MODELS_DIR}/output_hatexplain_bert-base-uncased_0001_SEED0042/checkpoint-2405\", \"hatexplain\", \"validation\"),\n",
    "#     (f\"{MODELS_DIR}/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-16370\", \"qnli\", \"validation\"),\n",
    "#     (f\"{MODELS_DIR}/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-61360\", \"mnli\", \"validation_matched\"),\n",
    "#     (f\"{MODELS_DIR}/output_cola_bert-base-uncased_0001_SEED0042/checkpoint-1340\", \"cola\", \"validation\"),\n",
    "#     (f\"{MODELS_DIR}/output_mrpc_bert-base-uncased_0001_SEED0042/checkpoint-575\", \"mrpc\", \"validation\"),\n",
    "#     (f\"{MODELS_DIR}/output_sst2_bert-large-uncased_0001_SEED0042/checkpoint-10525\", \"sst2\", \"validation\"),\n",
    "    \n",
    "### TRAINING MAPS\n",
    "#     (f\"{MODELS_DIR}/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-10525\", \"sst2\", \"train\"),\n",
    "#     (f\"{MODELS_DIR}/output_hatexplain_bert-base-uncased_0001_SEED0042/checkpoint-2405\", \"hatexplain\", \"train\"),\n",
    "#     (f\"{MODELS_DIR}/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-16370\", \"qnli\", \"train\"),\n",
    "#     (f\"{MODELS_DIR}/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-61360\", \"mnli\", \"train\"),\n",
    "#     (f\"{MODELS_DIR}/output_cola_bert-base-uncased_0001_SEED0042/checkpoint-1340\", \"cola\", \"train\"),\n",
    "#     (f\"{MODELS_DIR}/output_mrpc_bert-base-uncased_0001_SEED0042/checkpoint-575\", \"mrpc\", \"train\"),\n",
    "    \n",
    "### RoBERTa\n",
    "#     (f\"WillHeld/roberta-base-sst2\", \"sst2\", \"validation\"),\n",
    "    (f\"WillHeld/roberta-base-mnli\", \"mnli\", \"validation_matched\"),\n",
    "]\n",
    "\n",
    "GLOBENC_CONFIGS = {\n",
    "    \"GlobEnc\":\n",
    "        GlobencConfig(\n",
    "            include_biases=False,  ###\n",
    "            bias_decomp_type=\"absdot\",\n",
    "            include_LN1=True,\n",
    "            include_FFN=False,  ###\n",
    "            FFN_approx_type=\"GeLU_LA\",\n",
    "            include_LN2=True,\n",
    "            aggregation=\"rollout\",  ###\n",
    "            include_classifier_w_pooler=False,  ###\n",
    "            tanh_approx_type=\"ZO\",\n",
    "            output_all_layers=True,\n",
    "            output_attention=None,\n",
    "            output_res1=None,\n",
    "            output_LN1=None,\n",
    "            output_FFN=None,\n",
    "            output_res2=None,\n",
    "            output_encoder=None,\n",
    "            output_aggregated=\"norm\",\n",
    "            output_pooler=None,\n",
    "            output_classifier=False,\n",
    "        ),\n",
    "#     \"GlobEnc AbsDot Bias\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"absdot\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=False,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"rollout\",\n",
    "#             include_classifier_w_pooler=False,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=None,\n",
    "#             output_classifier=False,\n",
    "#         ),\n",
    "#     \"GlobEnc AbsSim Bias\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"abssim\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=False,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"rollout\",\n",
    "#             include_classifier_w_pooler=False,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=None,\n",
    "#             output_classifier=False,\n",
    "#         ),\n",
    "#     \"GlobEnc Equal Bias\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"equal\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=False,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"rollout\",\n",
    "#             include_classifier_w_pooler=False,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=None,\n",
    "#             output_classifier=False,\n",
    "#         ),\n",
    "#     \"GlobEnc Norm Bias\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"norm\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=False,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"rollout\",\n",
    "#             include_classifier_w_pooler=False,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=None,\n",
    "#             output_classifier=False,\n",
    "#         ),\n",
    "#     \"GlobEnc No Bias FFN\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=False,\n",
    "#             bias_decomp_type=\"absdot\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=True,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"rollout\",\n",
    "#             include_classifier_w_pooler=False,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=None,\n",
    "#             output_classifier=False,\n",
    "#         ),\n",
    "#     \"GlobEnc AbsDot Bias FFN\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"absdot\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=True,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"rollout\",\n",
    "#             include_classifier_w_pooler=False,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=None,\n",
    "#             output_classifier=False,\n",
    "#         ),\n",
    "#     \"GlobEnc Equal Bias FFN\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"equal\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=True,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"rollout\",\n",
    "#             include_classifier_w_pooler=False,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=None,\n",
    "#             output_classifier=False,\n",
    "#         ),\n",
    "#     \"GlobEnc AbsSim Bias FFN\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"abssim\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=True,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"rollout\",\n",
    "#             include_classifier_w_pooler=False,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=None,\n",
    "#             output_classifier=False,\n",
    "#         ),\n",
    "#     \"GlobEnc Norm Bias FFN\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"abssim\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=True,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"rollout\",\n",
    "#             include_classifier_w_pooler=False,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=None,\n",
    "#             output_classifier=False,\n",
    "#         ),\n",
    "#     \"Decomposition No Bias\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=False,\n",
    "#             bias_decomp_type=\"absdot\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=True,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"vector\",\n",
    "#             include_classifier_w_pooler=True,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=\"norm\",\n",
    "#             output_classifier=True,\n",
    "#         ),\n",
    "    \"Decomposition AbsDot Bias\":\n",
    "        GlobencConfig(\n",
    "            include_biases=True,\n",
    "            bias_decomp_type=\"absdot\",\n",
    "            include_LN1=True,\n",
    "            include_FFN=True,\n",
    "            FFN_approx_type=\"GeLU_ZO\",\n",
    "            include_LN2=True,\n",
    "            aggregation=\"vector\",\n",
    "            include_classifier_w_pooler=True,\n",
    "            tanh_approx_type=\"ZO\",\n",
    "            output_all_layers=True,\n",
    "            output_attention=None,\n",
    "            output_res1=None,\n",
    "            output_LN1=None,\n",
    "            output_FFN=None,\n",
    "            output_res2=None,\n",
    "            output_encoder=None,\n",
    "            output_aggregated=\"norm\",\n",
    "            output_pooler=\"norm\",\n",
    "            output_classifier=True,\n",
    "        ),\n",
    "#     \"Decomposition AbsDot Bias No FFN\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"absdot\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=False,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"vector\",\n",
    "#             include_classifier_w_pooler=True,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=\"norm\",\n",
    "#             output_classifier=True,\n",
    "#         ),\n",
    "#     \"Decomposition No Bias No FFN\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=False,\n",
    "#             bias_decomp_type=\"absdot\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=False,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"vector\",\n",
    "#             include_classifier_w_pooler=True,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=\"norm\",\n",
    "#             output_classifier=True,\n",
    "#         ),\n",
    "#     \"Decomposition AbsDot Bias ReLU\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"absdot\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=True,\n",
    "#             FFN_approx_type=\"ReLU\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"vector\",\n",
    "#             include_classifier_w_pooler=True,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=\"norm\",\n",
    "#             output_classifier=True,\n",
    "#         ),\n",
    "#     \"Decomposition Equal Bias\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"equal\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=True,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"vector\",\n",
    "#             include_classifier_w_pooler=True,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=\"norm\",\n",
    "#             output_classifier=True,\n",
    "#         ),\n",
    "#     \"Decomposition Norm Bias\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"norm\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=True,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"vector\",\n",
    "#             include_classifier_w_pooler=True,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=\"norm\",\n",
    "#             output_classifier=True,\n",
    "#         ),\n",
    "#     \"Decomposition AbsSim Bias\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"abssim\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=True,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"vector\",\n",
    "#             include_classifier_w_pooler=True,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=\"norm\",\n",
    "#             output_classifier=True,\n",
    "#         ),\n",
    "#     \"Decomposition Norm Bias Token\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"norm\",\n",
    "#             include_bias_token=True,\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=True,\n",
    "#             FFN_approx_type=\"GeLU_ZO\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"vector\",\n",
    "#             include_classifier_w_pooler=True,\n",
    "#             tanh_approx_type=\"ZO\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=\"norm\",\n",
    "#             output_classifier=True,\n",
    "#         ),\n",
    "    \n",
    "    \n",
    "#     \"GlobEnc FFN LinearApproximation\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"absdot\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=True,\n",
    "#             FFN_approx_type=\"GeLU_LA\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"rollout\",\n",
    "#             include_classifier_w_pooler=False,\n",
    "#             tanh_approx_type=\"LA\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=None,\n",
    "#             output_classifier=False,\n",
    "#         ),\n",
    "#     \"Decomposition LinearApproximation\":\n",
    "#         GlobencConfig(\n",
    "#             include_biases=True,\n",
    "#             bias_decomp_type=\"absdot\",\n",
    "#             include_LN1=True,\n",
    "#             include_FFN=True,\n",
    "#             FFN_approx_type=\"GeLU_LA\",\n",
    "#             include_LN2=True,\n",
    "#             aggregation=\"vector\",\n",
    "#             include_classifier_w_pooler=True,\n",
    "#             tanh_approx_type=\"LA\",\n",
    "#             output_all_layers=True,\n",
    "#             output_attention=None,\n",
    "#             output_res1=None,\n",
    "#             output_LN1=None,\n",
    "#             output_FFN=None,\n",
    "#             output_res2=None,\n",
    "#             output_encoder=None,\n",
    "#             output_aggregated=\"norm\",\n",
    "#             output_pooler=\"norm\",\n",
    "#             output_classifier=True,\n",
    "#         ),\n",
    "}\n",
    "\n",
    "SAVE_CLS = False\n",
    "SAVE_ALL_LAYERS = False  # 12 layers + last layer\n",
    "GLOBENC_CONFIGS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d2a2ce",
   "metadata": {},
   "source": [
    "# Retreive and Save\n",
    "Namings are all automatically determined from the configs.\n",
    "```\n",
    "file_name = f\"[{task_name}]_[{set_of_data}]_[{'-'.join(model_checkpoint.split('/')[-2:])}]_[{globenc_cfg_name}]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointToGlobenc:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_checkpoint: str,\n",
    "        globenc_config: GlobencConfig,\n",
    "        task_name: str,\n",
    "        set_of_data: str,\n",
    "        save_cls: bool = False,\n",
    "        save_all_layers: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "        self.globenc_config = globenc_config\n",
    "        self.task_name = task_name\n",
    "        self.set_of_data = set_of_data\n",
    "        self.save_cls = save_cls\n",
    "        self.save_all_layers = save_all_layers\n",
    "    \n",
    "    def retrieve(self) -> pd.DataFrame:\n",
    "        DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        ### DATASET ###\n",
    "        def aggregate_hatexplain(example):\n",
    "            def mode(lst):\n",
    "                return max(set(lst), key=lst.count)\n",
    "            example[\"label\"] = mode(example[\"annotators\"][\"label\"])\n",
    "            example[\"text\"] = \" \".join(example[\"post_tokens\"])\n",
    "            return example\n",
    "        GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
    "        BATCH_SIZE = 8\n",
    "        MAX_LENGTH = 128\n",
    "        actual_task = \"mnli\" if self.task_name == \"mnli-mm\" else self.task_name\n",
    "        if self.task_name in GLUE_TASKS:\n",
    "            dataset = datasets.load_dataset(\"glue\", actual_task, download_config=datasets.DownloadConfig(local_files_only=True))\n",
    "        elif self.task_name == \"hatexplain\":\n",
    "            dataset = datasets.load_dataset(\"hatexplain\", download_config=datasets.DownloadConfig(local_files_only=True)).map(aggregate_hatexplain)\n",
    "            for part in [\"train\", \"validation\", \"test\"]:\n",
    "                dataset[part] = dataset[part].add_column(\"idx\", [i for i in range(len(dataset[part]))])\n",
    "        task_to_keys = {\n",
    "            \"cola\": (\"sentence\", None),\n",
    "            \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "            \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "            \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "            \"qnli\": (\"question\", \"sentence\"),\n",
    "            \"qqp\": (\"question1\", \"question2\"),\n",
    "            \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "            \"sst2\": (\"sentence\", None),\n",
    "            \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "            \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "            \"hatexplain\": (\"text\", None),\n",
    "        }\n",
    "        SENTENCE1_KEY, SENTENCE2_KEY = task_to_keys[self.task_name]\n",
    "        def preprocess_function_wrapped(tokenizer):\n",
    "            def preprocess_function(examples):\n",
    "                # Tokenize the texts\n",
    "                args = (\n",
    "                    (examples[SENTENCE1_KEY],) if SENTENCE2_KEY is None else (examples[SENTENCE1_KEY], examples[SENTENCE2_KEY])\n",
    "                )\n",
    "                result = tokenizer(*args, padding=False, max_length=MAX_LENGTH, truncation=True)\n",
    "                return result\n",
    "            return preprocess_function\n",
    "\n",
    "        def token_id_to_tokens_mapper(tokenizer, sample):\n",
    "            length = len(sample[\"input_ids\"])\n",
    "            return tokenizer.convert_ids_to_tokens(sample[\"input_ids\"])[:length], length\n",
    "        \n",
    "        ### RUN ###\n",
    "        if \"roberta\" in self.model_checkpoint:\n",
    "            model = RobertaForSequenceClassification.from_pretrained(self.model_checkpoint)\n",
    "        else:\n",
    "            model = BertForSequenceClassification.from_pretrained(self.model_checkpoint)\n",
    "        model.eval()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint, use_fast=True, max_length=128)\n",
    "\n",
    "        sel_dataset = dataset[self.set_of_data].map(preprocess_function_wrapped(tokenizer), batched=True, batch_size=1024)\n",
    "        dataset_size = len(sel_dataset)\n",
    "        steps = int(np.ceil(dataset_size / BATCH_SIZE))\n",
    "\n",
    "        final_data = {\n",
    "            \"tokens\": [],\n",
    "        }\n",
    "        lengths = []\n",
    "        for i in tqdm(range(dataset_size), desc=\"Tokenize\"):\n",
    "            tokens, length = token_id_to_tokens_mapper(tokenizer, sel_dataset[i])\n",
    "            final_data[\"tokens\"].append(tokens)\n",
    "            lengths.append(length)\n",
    "\n",
    "        generator = torch.Generator()\n",
    "        generator.manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n",
    "\n",
    "        sampler = LengthGroupedSampler(\n",
    "            BATCH_SIZE,\n",
    "            lengths=lengths,\n",
    "            model_input_name=tokenizer.model_input_names[0],\n",
    "            generator=generator,\n",
    "        )\n",
    "\n",
    "        collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "        sel_dataset = sel_dataset.add_column(\"length\", lengths)\n",
    "        cols = [\"input_ids\", \"attention_mask\", \"length\", \"idx\"]\n",
    "        cols = cols + [\"token_type_ids\"] if not \"roberta\" in self.model_checkpoint else cols\n",
    "        sel_dataset.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            sel_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            sampler=sampler,\n",
    "            collate_fn=collator\n",
    "        )\n",
    "\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        it = iter(dataloader)\n",
    "\n",
    "        idxes = []\n",
    "        shuffled_globencs, shuffled_cls, shuffled_globencs_all_layers = [], [], []\n",
    "        shuffled_data = {\n",
    "            \"importance_last_layer_aggregated\": [],\n",
    "            \"importance_last_layer_pooler\": [],\n",
    "            \"importance_last_layer_classifier\": [],\n",
    "            \"importance_all_layers_aggregated\": [],\n",
    "            \"cls\": [],\n",
    "            \"logits\": [],\n",
    "            \"label\": [],\n",
    "        }\n",
    "        first_time = datetime.datetime.now()\n",
    "        with torch.no_grad():\n",
    "            for i in tqdm(range(steps), desc=\"Forward\"):\n",
    "                batch = next(it)\n",
    "#                 print(\"#################\")\n",
    "#                 print(batch.keys())['idx', 'input_ids', 'attention_mask', 'length']\n",
    "#                 print(\"#################\")\n",
    "                input_batch = {k: batch[k].to(DEVICE) for k in batch.keys() - ['idx', 'length']}\n",
    "                logits, hidden_states, globenc_last_layer_outputs, globenc_all_layers_outputs = model(\n",
    "                    **input_batch, \n",
    "                    output_attentions=False, \n",
    "                    return_dict=False, \n",
    "                    output_hidden_states=True, \n",
    "                    globenc_config=self.globenc_config\n",
    "                )\n",
    "                # globenc_last_layer_outputs.aggregated ~ (1, 8, 55, 55)\n",
    "                # globenc_last_layer_outputs.pooler ~ (1, 8, 55)\n",
    "                # globenc_last_layer_outputs.classifier ~ (8, 55, 2)\n",
    "                # globenc_all_layers_outputs.aggregated ~ (12, 8, 55, 55)\n",
    "                # logits ~ (8, 2)\n",
    "                # hidden_states ~ (13, 8, 55, 768)\n",
    "                \n",
    "                batch_lengths = batch[\"length\"].numpy()\n",
    "                idxes.extend(batch['idx'].tolist())\n",
    "                \n",
    "                ### logits ~ (8, 2) ###\n",
    "                shuffled_data[\"logits\"].extend(logits.cpu().numpy())\n",
    "                \n",
    "                ### globenc_last_layer_outputs.aggregated ~ (1, 8, 55, 55) ###\n",
    "                importance = np.array([g.squeeze().cpu().numpy() for g in globenc_last_layer_outputs.aggregated]).squeeze()  # (batch, seq_len, seq_len)\n",
    "                importance = [importance[j][:batch_lengths[j],:batch_lengths[j]] for j in range(len(importance))]\n",
    "                shuffled_data[\"importance_last_layer_aggregated\"].extend(importance)\n",
    "                \n",
    "                ### globenc_last_layer_outputs.pooler ~ (1, 8, 55) ###\n",
    "                if globenc_last_layer_outputs.pooler is not None:\n",
    "                    importance = np.array([g.squeeze().cpu().numpy() for g in globenc_last_layer_outputs.pooler]).squeeze()  # (batch, seq_len)\n",
    "                    importance = [importance[j][:batch_lengths[j]] for j in range(len(importance))]\n",
    "                    shuffled_data[\"importance_last_layer_pooler\"].extend(importance)\n",
    "                \n",
    "                ### globenc_last_layer_outputs.classifier ~ (8, 55, 2) ###\n",
    "                if globenc_last_layer_outputs.classifier is not None:\n",
    "                    importance = np.array([g.squeeze().cpu().numpy() for g in globenc_last_layer_outputs.classifier]).squeeze()  # (batch, seq_len, classes)\n",
    "                    importance = [importance[j][:batch_lengths[j], :] for j in range(len(importance))]\n",
    "                    shuffled_data[\"importance_last_layer_classifier\"].extend(importance)\n",
    "                \n",
    "                ### globenc_all_layers_outputs.aggregated ~ (12, 8, 55, 55) ###\n",
    "                if self.save_all_layers:\n",
    "                    importance = np.array([g.squeeze().cpu().numpy() for g in globenc_all_layers_outputs.aggregated])  # (layers, batch, seq_len, seq_len)\n",
    "                    importance = np.einsum('lbij->blij', importance)  # (batch, layers, seq_len, seq_len)\n",
    "                    importance = [importance[j][:, :batch_lengths[j], :batch_lengths[j]] for j in range(len(importance))]\n",
    "                    shuffled_data[\"importance_all_layers_aggregated\"].extend(importance)\n",
    "                \n",
    "                ### hidden_states ~ (13, 8, 55, 768) ###\n",
    "                if self.save_cls:\n",
    "                    shuffled_data[\"cls\"].extend(hidden_states[-1][:, 0, :].cpu().numpy())  # Last layer & only CLS -> (8, 768)\n",
    "        later_time = datetime.datetime.now()\n",
    "        # Reorder retrieved data\n",
    "        inverse_idxes = np.argsort(idxes)\n",
    "        for key in shuffled_data.keys():\n",
    "            if len(shuffled_data[key]) == 0:\n",
    "                shuffled_data[key] = [None for _ in range(dataset_size)]\n",
    "            final_data[key] = [shuffled_data[key][inverse_idxes[i]] for i in range(dataset_size)]\n",
    "            \n",
    "        ### labels ###\n",
    "        final_data[\"label\"] = sel_dataset[\"label\"]\n",
    "        df = pd.DataFrame(final_data)\n",
    "        df.attrs[\"time\"] = later_time - first_time\n",
    "        df.attrs[\"time/example\"] = df.attrs[\"time\"] / len(sel_dataset)\n",
    "        return df\n",
    "    #     save_pickle(globencs, f\"{OUTPUT_DIR}/{name}_{SET}_{list(MODELS.values())[0].replace('/', '-')}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45841aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_dataset_set in tqdm(MODEL_DATASET_SET, desc=\"Models_Dataset_Sets\"):\n",
    "    model_checkpoint, task_name, set_of_data = model_dataset_set\n",
    "    for globenc_cfg_name, globenc_cfg in tqdm(GLOBENC_CONFIGS.items(), desc=\"Globenc Configs\"):\n",
    "        torch.cuda.empty_cache()\n",
    "        try:\n",
    "            file_name = f\"[{task_name}]_[{set_of_data}]_[{'-'.join(model_checkpoint.split('/')[-2:])}]_[{globenc_cfg_name}]\"\n",
    "            print(f\"### {file_name}\")\n",
    "            ctg = CheckpointToGlobenc(\n",
    "                model_checkpoint=model_checkpoint,\n",
    "                globenc_config=globenc_cfg,\n",
    "                task_name=task_name,\n",
    "                set_of_data=set_of_data,\n",
    "                save_cls=SAVE_CLS,\n",
    "                save_all_layers=SAVE_ALL_LAYERS\n",
    "            )\n",
    "            df = ctg.retrieve()\n",
    "            display(df.head(1))\n",
    "            df.attrs[\"model_checkpoint\"] = model_checkpoint\n",
    "            df.attrs[\"task_name\"] = task_name\n",
    "            df.attrs[\"set_of_data\"] = set_of_data\n",
    "            df.attrs[\"save_cls\"] = SAVE_CLS\n",
    "            df.attrs[\"save_all_layers\"] = SAVE_ALL_LAYERS\n",
    "            df.attrs[\"globenc_config_name\"] = globenc_cfg_name\n",
    "            df.attrs[\"globenc_config\"] = globenc_cfg.__dict__\n",
    "            pathlib.Path(os.path.dirname(f\"{OUTPUT_DIR}/{file_name}.pkl\")).mkdir(parents=True, exist_ok=True) \n",
    "            df.to_pickle(f\"{OUTPUT_DIR}/{file_name}.pkl\")  # Can add zip/bz2/...\n",
    "            print(f\"{OUTPUT_DIR}/{file_name}.pkl\")\n",
    "            print(df.attrs)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1909cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"glue\", \"sst2\", download_config=datasets.DownloadConfig(local_files_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fd8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from transformers import EvalPrediction\n",
    "\n",
    "# def mode(lst):\n",
    "#     return max(set(lst), key=lst.count)\n",
    "\n",
    "# def update_data(example):\n",
    "#     example[\"label\"] = mode(example[\"annotators\"][\"label\"])\n",
    "#     example[\"text\"] = \" \".join(example[\"post_tokens\"])\n",
    "#     return example\n",
    "\n",
    "# ds = load_dataset(\"hatexplain\").map(update_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26abbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"glue\", \"sst2\", download_config=datasets.DownloadConfig(local_files_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb320ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034df31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
