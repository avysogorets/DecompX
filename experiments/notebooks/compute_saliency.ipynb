{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53265167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 20:52:06.432151: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-20 20:52:06.607363: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-20 20:52:07.199494: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/modaresi/.conda/envs/globenc-venv/lib/\n",
      "2023-01-20 20:52:07.199613: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/modaresi/.conda/envs/globenc-venv/lib/\n",
      "2023-01-20 20:52:07.199620: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from GlobEnc.src.modeling.modeling_roberta_saliency import RobertaForSequenceClassification\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e40a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"/home/modaresi/projects/globenc_analysis/outputs/models\"\n",
    "outputs_dir = \"/home/modaresi/projects/globenc_analysis/outputs/saliencies_angle\"\n",
    "configs = {\n",
    "    \"mnli\": {\n",
    "        \"model_path\": lambda step: f\"{models_dir}/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-{step}/\",\n",
    "        \"output_file_path\": lambda step: f\"{outputs_dir}/mnli_bert-base-uncased_0001_SEED0042_checkpoint-{step}.npy\",\n",
    "        \"hf_ds\": \"mnli\",\n",
    "    },\n",
    "    \"sst2\": {\n",
    "        \"model_path\": lambda step: f\"{models_dir}/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-{step}/\",\n",
    "        \"output_file_path\": lambda step: f\"{outputs_dir}/sst2_bert-base-uncased_0001_SEED0042_checkpoint-{step}.npy\",\n",
    "        \"hf_ds\": \"sst2\",\n",
    "    }\n",
    "}\n",
    "\n",
    "CONFIG_NAME = \"mnli\"\n",
    "CONFIG = configs[CONFIG_NAME]\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "DATA_SECTION = \"validation_matched\"\n",
    "# STEP = 10525\n",
    "STEP = 61360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f73b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/opt/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc15fbf5d6654bcab4caaa373938e51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "    num_rows: 9815\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_ds = datasets.load_dataset(\"glue\", CONFIG[\"hf_ds\"])[DATA_SECTION]\n",
    "original_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbb87862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418f2db1a9ba4e63b01af51f85f3b248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "def _get_preprocessing_function(\n",
    "    sentence1_key: str, \n",
    "    sentence2_key: str = None, \n",
    "    label_to_id: dict = None):\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize the texts\n",
    "        args = (\n",
    "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(*args, padding=False, max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "        # Map labels to IDs (not necessary for GLUE tasks)\n",
    "        if label_to_id is not None and \"label\" in examples:\n",
    "            result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "        return result\n",
    "    \n",
    "    return preprocess_function\n",
    "\n",
    "preprocess_function = _get_preprocessing_function(sentence1_key=\"premise\", sentence2_key=\"hypothesis\")\n",
    "# preprocess_function = _get_preprocessing_function(sentence1_key=\"sentence\")\n",
    "train_ds = original_ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27dccb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "collator = DataCollatorWithPadding(tokenizer, True, MAX_LENGTH, return_tensors=\"pt\")\n",
    "dataloader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "dataset_size = len(train_ds)\n",
    "steps = int(np.ceil(dataset_size / BATCH_SIZE))\n",
    "num_labels = len(set(original_ds['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac2fc2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b321eaab13d3418899e94c6803e3e9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modaresi/.conda/envs/globenc-venv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2280: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(CONFIG[\"model_path\"](STEP))\n",
    "model.to(torch.device(\"cuda:0\"))\n",
    "model.eval()\n",
    "\n",
    "all_sals = torch.zeros(size=(dataset_size, MAX_LENGTH)).cuda()\n",
    "it = iter(dataloader)\n",
    "\n",
    "for i in tqdm(range(steps)):\n",
    "    batch = next(it)\n",
    "    batch = {k: v.to(torch.device('cuda:0')) for k, v in batch.items()}\n",
    "    inputs = {\n",
    "        'input_ids': batch['input_ids'],\n",
    "        'attention_mask': batch['attention_mask'],\n",
    "        'token_type_ids': batch['token_type_ids'],\n",
    "    }\n",
    "    labels = batch['labels']\n",
    "    output = model(**batch, output_hidden_states=True)\n",
    "    \n",
    "    output.hidden_states[0].retain_grad()\n",
    "    logits = output.logits\n",
    "    target_class_l_sum = torch.gather(logits, 1, labels.unsqueeze(-1)).sum()\n",
    "    target_class_l_sum.backward()\n",
    "    \n",
    "    inputXgradient = output.hidden_states[0].grad * output.hidden_states[0]\n",
    "    # saliencies = torch.norm(inputXgradient, dim=-1).detach()\n",
    "    saliencies = torch.sum(inputXgradient, dim=-1).detach()\n",
    "    \n",
    "    length = saliencies.size()[1]\n",
    "    model.zero_grad()\n",
    "    all_sals[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :length] = saliencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62b325b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sals = all_sals.cpu().numpy()\n",
    "np.save(CONFIG[\"output_file_path\"](STEP), all_sals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fcf1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4bff92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ecb0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"/home/modaresi/projects/globenc_analysis/outputs/models\"\n",
    "outputs_dir = \"/home/modaresi/projects/globenc_analysis/outputs/saliencies\"\n",
    "\n",
    "# configs = {\n",
    "#     \"mnli\": {\n",
    "#         \"model_path\": lambda step: f\"{models_dir}/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-{step}/\",\n",
    "#         \"output_file_path\": lambda step, agg_t, bl_t: f\"{outputs_dir}/mnli_bert-base-uncased_0001_SEED0042_checkpoint-{step}-{agg_t}-{bl_t}.npy\",\n",
    "#         \"hf_ds\": \"mnli\",\n",
    "#     },\n",
    "#     \"sst2\": {\n",
    "#         \"model_path\": lambda step: f\"{models_dir}/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-{step}/\",\n",
    "#         \"output_file_path\": lambda step, agg_t, bl_t: f\"{outputs_dir}/sst2_bert-base-uncased_0001_SEED0042_checkpoint-{step}-{agg_t}-{bl_t}.npy\",\n",
    "#         \"hf_ds\": \"sst2\",\n",
    "#     }\n",
    "# }\n",
    "\n",
    "MODEL_DATASET_SET_PARTS = [\n",
    "#     (f\"{models_dir}/output_cola_bert-base-uncased_0001_SEED0042/checkpoint-1340\", \"cola\", \"validation\"),\n",
    "#     (f\"{models_dir}/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-10525\", \"sst2\", \"train\", [\"sentence\"]),\n",
    "#     (f\"{models_dir}/output_sst2_bert-large-uncased_0001_SEED0042/checkpoint-10525\", \"sst2\", \"validation\"),\n",
    "#     (f\"{models_dir}/output_mrpc_bert-base-uncased_0001_SEED0042/checkpoint-575\", \"mrpc\", \"validation\"),\n",
    "#     (f\"{models_dir}/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-16370\", \"qnli\", \"train\", [\"question\", \"sentence\"]),\n",
    "#     (f\"{models_dir}/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-61360\", \"mnli\", \"train\", [\"premise\", \"hypothesis\"]),\n",
    "#     (f\"{models_dir}/output_hatexplain_bert-base-uncased_0001_SEED0042/checkpoint-2405\", \"hatexplain\", \"train\", [\"text\"]),\n",
    "    (f\"WillHeld/roberta-base-sst2\", \"sst2\", \"validation\", [\"sentence\"]),\n",
    "    (f\"WillHeld/roberta-base-mnli\", \"mnli\", \"validation_matched\", [\"premise\", \"hypothesis\"]),\n",
    "]\n",
    "\n",
    "# CONFIG_NAME = \"mnli\"\n",
    "# CONFIG = configs[CONFIG_NAME]\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "# DATA_SECTION = \"validation_matched\"\n",
    "AGGREGATION_TYPES = [\"NORM\", \"SUM\"]\n",
    "# STEP = 10525\n",
    "# STEP = 61360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29406fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_preprocessing_function(\n",
    "    sentence1_key: str, \n",
    "    sentence2_key: str = None, \n",
    "    label_to_id: dict = None):\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize the texts\n",
    "        args = (\n",
    "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(*args, padding=False, max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "        # Map labels to IDs (not necessary for GLUE tasks)\n",
    "        if label_to_id is not None and \"label\" in examples:\n",
    "            result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "        return result\n",
    "    \n",
    "    return preprocess_function\n",
    "\n",
    "def aggregate_hatexplain(example):\n",
    "    def mode(lst):\n",
    "        return max(set(lst), key=lst.count)\n",
    "    example[\"label\"] = mode(example[\"annotators\"][\"label\"])\n",
    "    example[\"text\"] = \" \".join(example[\"post_tokens\"])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33e59f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saliencies(dataloader, steps, model, aggregation_type=None, prediction_based=True):\n",
    "    all_sals_norm_aggregated = np.zeros(shape=(dataset_size, MAX_LENGTH))\n",
    "    all_sals_sum_aggregated = np.zeros(shape=(dataset_size, MAX_LENGTH))\n",
    "    it = iter(dataloader)\n",
    "    \n",
    "    for i in tqdm(range(steps)):\n",
    "        batch = next(it)\n",
    "        batch = {k: v.to(torch.device('cuda:0')) for k, v in batch.items()}\n",
    "        inputs = {\n",
    "            'input_ids': batch['input_ids'],\n",
    "            'attention_mask': batch['attention_mask'],\n",
    "#             'token_type_ids': batch['token_type_ids'],\n",
    "        }\n",
    "        labels = batch['labels']\n",
    "        output = model(**batch, output_hidden_states=True)\n",
    "\n",
    "        output.hidden_states[0].retain_grad()\n",
    "        logits = output.logits\n",
    "        if prediction_based:\n",
    "            target_class_l_sum = torch.gather(logits, 1, torch.argmax(logits, dim=-1).unsqueeze(-1)).sum()\n",
    "        else:\n",
    "            target_class_l_sum = torch.gather(logits, 1, labels.unsqueeze(-1)).sum()\n",
    "        target_class_l_sum.backward()\n",
    "\n",
    "        inputXgradient = output.hidden_states[0].grad * output.hidden_states[0]\n",
    "        # saliencies = torch.norm(inputXgradient, dim=-1).detach()\n",
    "#         saliencies = torch.sum(inputXgradient, dim=-1).detach()\n",
    "#         if aggregation_type == \"SUM\":\n",
    "#             saliencies = torch.sum(inputXgradient, dim=-1).detach().cpu()\n",
    "#         elif aggregation_type == \"NORM\":\n",
    "#             saliencies = torch.norm(inputXgradient, dim=-1).detach().cpu()\n",
    "        saliencies_norm_aggregated = torch.norm(inputXgradient, dim=-1).detach().cpu()\n",
    "        saliencies_sum_aggregated = torch.sum(inputXgradient, dim=-1).detach().cpu()\n",
    "\n",
    "        length = saliencies_norm_aggregated.size()[1]\n",
    "        model.zero_grad()\n",
    "        all_sals_norm_aggregated[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :length] = saliencies_norm_aggregated\n",
    "        all_sals_sum_aggregated[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :length] = saliencies_sum_aggregated\n",
    "#         all_sals[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :length] = saliencies\n",
    "        \n",
    "    return all_sals_norm_aggregated, all_sals_sum_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99cd3a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /opt/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad (last modified on Sat Sep  3 17:51:06 2022) since it couldn't be found locally at glue., or remotely on the Hugging Face Hub.\n",
      "Reusing dataset glue (/opt/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50d05e704e44b95ac03d280023b6ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2187485ccf4416eae7931be792a9e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dcb46d44214151a73e105a404c4a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /opt/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad (last modified on Sat Sep  3 17:51:06 2022) since it couldn't be found locally at glue., or remotely on the Hugging Face Hub.\n",
      "Reusing dataset glue (/opt/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fc9e2280ea4ae8902396f4d2ced83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bb6604b0a44736b6ec67d11895af30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebfa7db9bd94bc19d8ebfc1f36dbcb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_reports = dict()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "for model_dataset_set_parts in MODEL_DATASET_SET_PARTS:\n",
    "    model_checkpoint, task_name, set_of_data, sample_parts = model_dataset_set_parts\n",
    "#     model = BertForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "    model.to(torch.device(\"cuda:0\"))\n",
    "    model.eval()\n",
    "    \n",
    "    if task_name == \"hatexplain\":\n",
    "        original_ds = datasets.load_dataset(task_name)[set_of_data].map(aggregate_hatexplain)\n",
    "        preprocess_function = _get_preprocessing_function(sentence1_key=sample_parts[0])\n",
    "        train_ds = original_ds.map(preprocess_function, batched=True)\n",
    "    else:\n",
    "        original_ds = datasets.load_dataset(\"glue\", task_name)[set_of_data]\n",
    "        if len(sample_parts) == 1:\n",
    "            preprocess_function = _get_preprocessing_function(sentence1_key=sample_parts[0])\n",
    "        elif len(sample_parts) == 2:\n",
    "            preprocess_function = _get_preprocessing_function(sentence1_key=sample_parts[0], sentence2_key=sample_parts[1])\n",
    "        train_ds = original_ds.map(preprocess_function, batched=True)\n",
    "    \n",
    "#     train_ds.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "    train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    collator = DataCollatorWithPadding(tokenizer, True, MAX_LENGTH, return_tensors=\"pt\")\n",
    "    dataloader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "    dataset_size = len(train_ds)\n",
    "    steps = int(np.ceil(dataset_size / BATCH_SIZE))\n",
    "    num_labels = len(set(original_ds['label']))\n",
    "    \n",
    "#     for agg_t in AGGREGATION_TYPES:\n",
    "#         for gradient_base in [\"prediction_based\", \"label_based\"]:\n",
    "    for gradient_base in [\"prediction_based\"]:\n",
    "        t1 = time.time()\n",
    "        saliencies_norm_aggregated, saliencies_sum_aggregated = get_saliencies(\n",
    "            dataloader=dataloader,\n",
    "            steps=steps,\n",
    "            model=model,\n",
    "#             aggregation_type=agg_t,\n",
    "            prediction_based=(gradient_base==\"prediction_based\")\n",
    "        )\n",
    "        t2 = time.time()\n",
    "\n",
    "        time_reports[f\"{task_name}-{set_of_data}-{AGGREGATION_TYPES[0]}-{gradient_base}\"] = t2 - t1\n",
    "\n",
    "        file_name_norm_aggregated = f\"[{task_name}]_[{set_of_data}]_[{model_checkpoint.split('/')[-1]}]_[IXG_{AGGREGATION_TYPES[0]}_{gradient_base}]\"\n",
    "        file_name_sum_aggregated = f\"[{task_name}]_[{set_of_data}]_[{model_checkpoint.split('/')[-1]}]_[IXG_{AGGREGATION_TYPES[1]}_{gradient_base}]\"\n",
    "        \n",
    "#         np.save(f\"{outputs_dir}/{file_name}.npy\", saliencies)\n",
    "        np.save(f\"{outputs_dir}/{file_name_norm_aggregated}.npy\", saliencies_norm_aggregated)\n",
    "        np.save(f\"{outputs_dir}/{file_name_sum_aggregated}.npy\", saliencies_sum_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63024edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89108ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad96aab5654ccd4e66bf13bd728d9d4512a27ebcee40fc973a7275c9c55ebd75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
