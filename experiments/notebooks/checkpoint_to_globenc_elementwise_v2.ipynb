{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'GlobEnc' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "# ! pip install datasets==1.18.3\n",
    "# ! pip install transformers==4.18.0\n",
    "! git clone https://github.com/mohsenfayyaz/GlobEnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import pickle\n",
    "import pathlib\n",
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers.trainer_pt_utils import LengthGroupedSampler\n",
    "\n",
    "from GlobEnc.src.modeling.modeling_bert_elementwise import BertForSequenceClassification\n",
    "# from GlobEnc.src.modeling.modeling_electra import ElectraForSequenceClassification\n",
    "# from GlobEnc.src.attention_rollout import AttentionRollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_DIR = \"/home/modaresi/projects/globenc_analysis/outputs/globencs_elementwise\"\n",
    "# MODELS = {\n",
    "#     \"sst2-e0\": \"bert-large-uncased\",\n",
    "#     \"sst2-e1\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_sst2_bert-large-uncased_0001_SEED0042/checkpoint-2105\",\n",
    "#     \"sst2-e2\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_sst2_bert-large-uncased_0001_SEED0042/checkpoint-4210\",\n",
    "#     \"sst2-e3\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_sst2_bert-large-uncased_0001_SEED0042/checkpoint-6315\",\n",
    "#     \"sst2-e4\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_sst2_bert-large-uncased_0001_SEED0042/checkpoint-8420\",\n",
    "#     \"sst2-e5\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_sst2_bert-large-uncased_0001_SEED0042/checkpoint-10525\",\n",
    "# }\n",
    "MODELS = {\n",
    "#     \"sst2-e0\": \"bert-base-uncased\",\n",
    "#     \"sst2-e1\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-2105\",\n",
    "#     \"sst2-e2\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-4210\",\n",
    "#     \"sst2-e3\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-6315\",\n",
    "#     \"sst2-e4\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-8420\",\n",
    "    \"sst2-e5\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-10525\",\n",
    "}\n",
    "# MODELS = {\n",
    "#     \"mnli-e0\": \"bert-base-uncased\",\n",
    "#     \"mnli-e1\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-12272\",\n",
    "#     \"mnli-e2\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-24544\",\n",
    "#     \"mnli-e3\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-36816\",\n",
    "#     \"mnli-e4\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-49088\",\n",
    "#     \"mnli-e5\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-61360\",\n",
    "# }\n",
    "# MODELS = {\n",
    "#     \"cola-e0\": \"bert-base-uncased\",\n",
    "#     \"cola-e1\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_cola_bert-base-uncased_0001_SEED0042/checkpoint-268\",\n",
    "#     \"cola-e2\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_cola_bert-base-uncased_0001_SEED0042/checkpoint-536\",\n",
    "#     \"cola-e3\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_cola_bert-base-uncased_0001_SEED0042/checkpoint-804\",\n",
    "#     \"cola-e4\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_cola_bert-base-uncased_0001_SEED0042/checkpoint-1072\",\n",
    "#     \"cola-e5\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_cola_bert-base-uncased_0001_SEED0042/checkpoint-1340\",\n",
    "# }\n",
    "# MODELS = {\n",
    "#     \"mrpc-e0\": \"bert-base-uncased\",\n",
    "#     \"mrpc-e1\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_mrpc_bert-base-uncased_0001_SEED0042/checkpoint-115\",\n",
    "#     \"mrpc-e2\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_mrpc_bert-base-uncased_0001_SEED0042/checkpoint-230\",\n",
    "#     \"mrpc-e3\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_mrpc_bert-base-uncased_0001_SEED0042/checkpoint-345\",\n",
    "#     \"mrpc-e4\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_mrpc_bert-base-uncased_0001_SEED0042/checkpoint-460\",\n",
    "#     \"mrpc-e5\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_mrpc_bert-base-uncased_0001_SEED0042/checkpoint-575\",\n",
    "# }\n",
    "# MODELS = {\n",
    "#     \"qnli-e0\": \"bert-base-uncased\",\n",
    "#     \"qnli-e1\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-3274\",\n",
    "#     \"qnli-e2\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-6548\",\n",
    "#     \"qnli-e3\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-9822\",\n",
    "#     \"qnli-e4\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-13096\",\n",
    "#     \"qnli-e5\": \"/home/modaresi/projects/globenc_analysis/outputs/models/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-16370\",\n",
    "# }\n",
    "\n",
    "# MULTI BERT\n",
    "# MODELS = {\n",
    "#     \"multibert-qnli-s0\": \"google/multiberts-seed_0\",\n",
    "#     \"multibert-qnli-s1\": \"google/multiberts-seed_1\",\n",
    "#     \"multibert-qnli-s2\": \"google/multiberts-seed_2\",\n",
    "#     \"multibert-qnli-s3\": \"google/multiberts-seed_3\",\n",
    "#     \"multibert-qnli-s4\": \"google/multiberts-seed_4\",\n",
    "#     \"multibert-qnli-s5\": \"google/multiberts-seed_5\",\n",
    "# }\n",
    "\n",
    "# MODELS = {\n",
    "#     \"multibert-mrpc-s0\": \"google/multiberts-seed_0\",\n",
    "#     \"multibert-mrpc-s1\": \"google/multiberts-seed_1\",\n",
    "#     \"multibert-mrpc-s2\": \"google/multiberts-seed_2\",\n",
    "#     \"multibert-mrpc-s3\": \"google/multiberts-seed_3\",\n",
    "#     \"multibert-mrpc-s4\": \"google/multiberts-seed_4\",\n",
    "#     \"multibert-mrpc-s5\": \"google/multiberts-seed_5\",\n",
    "# }\n",
    "\n",
    "# MODELS = {\n",
    "#     \"multibert-cola-s0\": \"google/multiberts-seed_0\",\n",
    "#     \"multibert-cola-s1\": \"google/multiberts-seed_1\",\n",
    "#     \"multibert-cola-s2\": \"google/multiberts-seed_2\",\n",
    "#     \"multibert-cola-s3\": \"google/multiberts-seed_3\",\n",
    "#     \"multibert-cola-s4\": \"google/multiberts-seed_4\",\n",
    "#     \"multibert-cola-s5\": \"google/multiberts-seed_5\",\n",
    "# }\n",
    "\n",
    "# MODELS = {\n",
    "#     \"multibert-qnli-e0\": \"google/multiberts-seed_0-step_0k\",\n",
    "#     \"multibert-qnli-e1\": \"google/multiberts-seed_0-step_400k\",\n",
    "#     \"multibert-qnli-e2\": \"google/multiberts-seed_0-step_800k\",\n",
    "#     \"multibert-qnli-e3\": \"google/multiberts-seed_0-step_1200k\",\n",
    "#     \"multibert-qnli-e4\": \"google/multiberts-seed_0-step_1600k\",\n",
    "#     \"multibert-qnli-e5\": \"google/multiberts-seed_0-step_2000k\",\n",
    "# }\n",
    "\n",
    "# MODELS = {\n",
    "#     \"multibert-sst2-e0-20k\": \"google/multiberts-seed_0-step_0k\",\n",
    "#     \"multibert-sst2-e1-20k\": \"google/multiberts-seed_0-step_20k\",\n",
    "#     \"multibert-sst2-e2-20k\": \"google/multiberts-seed_0-step_40k\",\n",
    "#     \"multibert-sst2-e3-20k\": \"google/multiberts-seed_0-step_60k\",\n",
    "#     \"multibert-sst2-e4-20k\": \"google/multiberts-seed_0-step_80k\",\n",
    "#     \"multibert-sst2-e5-20k\": \"google/multiberts-seed_0-step_100k\",\n",
    "# }\n",
    "\n",
    "# MODELS = {\n",
    "#     \"multibert-sst2-e0-400k\": \"google/multiberts-seed_0-step_0k\",\n",
    "#     \"multibert-sst2-e1-400k\": \"google/multiberts-seed_0-step_400k\",\n",
    "#     \"multibert-sst2-e2-400k\": \"google/multiberts-seed_0-step_800k\",\n",
    "#     \"multibert-sst2-e3-400k\": \"google/multiberts-seed_0-step_1200k\",\n",
    "#     \"multibert-sst2-e4-400k\": \"google/multiberts-seed_0-step_1600k\",\n",
    "#     \"multibert-sst2-e5-400k\": \"google/multiberts-seed_0-step_2000k\",\n",
    "# }\n",
    "\n",
    "TASK = \"sst2\"\n",
    " \n",
    "SET = \"validation\"  # train/validation/validation_matched\n",
    "SAVE_CLS = True\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/opt/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3090128854241b5b11098e17daa10d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "actual_task = \"mnli\" if TASK == \"mnli-mm\" else TASK\n",
    "dataset = datasets.load_dataset(\"glue\", actual_task)\n",
    "metric = datasets.load_metric('glue', actual_task)\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "SENTENCE1_KEY, SENTENCE2_KEY = task_to_keys[TASK]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_wrapped(tokenizer):\n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize the texts\n",
    "        args = (\n",
    "            (examples[SENTENCE1_KEY],) if SENTENCE2_KEY is None else (examples[SENTENCE1_KEY], examples[SENTENCE2_KEY])\n",
    "        )\n",
    "        result = tokenizer(*args, padding=False, max_length=MAX_LENGTH, truncation=True)\n",
    "        return result\n",
    "    return preprocess_function\n",
    "\n",
    "def token_id_to_tokens_mapper(tokenizer, sample):\n",
    "    length = len(sample[\"input_ids\"])\n",
    "    return tokenizer.convert_ids_to_tokens(sample[\"input_ids\"])[:length], length\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    pathlib.Path(os.path.dirname(path)).mkdir(parents=True, exist_ok=True) \n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"Saved {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089a87e24f654c3d9e95f36454eb2748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Models:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function_wrapped.<locals>.preprocess_function at 0x7f3d06648710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8490deae53c47c8acf945bf240ce256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd7ab73f68b4a4d849c06ca29ee4675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenize:   0%|          | 0/872 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4969d68c8e4fcaaf15bf4b7aa3b883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlobEnc:   0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /home/modaresi/projects/globenc_analysis/outputs/globencs_elementwise/sst2-e5_validation_-home-modaresi-projects-globenc_analysis-outputs-models-output_sst2_bert-base-uncased_0001_SEED0042-checkpoint-10525.pickle\n"
     ]
    }
   ],
   "source": [
    "for name, path in tqdm(MODELS.items(), desc=\"Models\"):\n",
    "    model = BertForSequenceClassification.from_pretrained(path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path, use_fast=True, max_length=MAX_LENGTH)\n",
    "\n",
    "    sel_dataset = dataset[SET].map(preprocess_function_wrapped(tokenizer), batched=True, batch_size=1024)\n",
    "    dataset_size = len(sel_dataset)\n",
    "    steps = int(np.ceil(dataset_size / BATCH_SIZE))\n",
    "\n",
    "    globencs = {\"globenc\": [], \"tokens\": [], \"cls\": []}\n",
    "    lengths = []\n",
    "\n",
    "    for i in tqdm(range(dataset_size), desc=\"Tokenize\"):\n",
    "        tokens, length = token_id_to_tokens_mapper(tokenizer, sel_dataset[i])\n",
    "        globencs[\"tokens\"].append(tokens)\n",
    "        lengths.append(length)\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n",
    "\n",
    "    sampler = LengthGroupedSampler(\n",
    "        BATCH_SIZE,\n",
    "        lengths=lengths,\n",
    "        model_input_name=tokenizer.model_input_names[0],\n",
    "        generator=generator,\n",
    "    )\n",
    "\n",
    "    collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    sel_dataset = sel_dataset.add_column(\"length\", lengths)\n",
    "    sel_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"length\", \"idx\"])\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "                sel_dataset,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                sampler=sampler,\n",
    "                collate_fn=collator\n",
    "    )\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    it = iter(dataloader)\n",
    "\n",
    "    idxes = []\n",
    "    shuffled_globencs, shuffled_cls = [], []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(steps), desc=\"GlobEnc\"):\n",
    "            batch = next(it)\n",
    "            input_batch = {k: batch[k].to(DEVICE) for k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]}\n",
    "            if SAVE_CLS:\n",
    "                logits, hidden_states, norms = model(**input_batch, output_attentions=False, output_norms=False, \n",
    "                                                     return_dict=False, output_globenc=True, output_hidden_states=True)\n",
    "            else:\n",
    "                logits, norms = model(**input_batch, output_attentions=False, output_norms=False, \n",
    "                                                     return_dict=False, output_globenc=True, output_hidden_states=False)\n",
    "            globenc = norms.squeeze().cpu().numpy()\n",
    "            batch_lengths = batch[\"length\"].numpy()\n",
    "            idxes.extend(batch['idx'].tolist())\n",
    "            shuffled_globencs.extend([globenc[j][:batch_lengths[j],:batch_lengths[j]] for j in range(len(globenc))])\n",
    "            \n",
    "            if SAVE_CLS:\n",
    "                cls_repr = hidden_states[-1][:, 0, :].cpu().numpy()  # [13, batch, len, 768]\n",
    "                shuffled_cls.extend(cls_repr)\n",
    "    inverse_idxes = np.argsort(idxes)\n",
    "    globencs[\"globenc\"] = [shuffled_globencs[inverse_idxes[i]] for i in range(dataset_size)]\n",
    "    if SAVE_CLS:\n",
    "        globencs[\"cls\"] = [shuffled_cls[inverse_idxes[i]] for i in range(dataset_size)]\n",
    "    save_pickle(globencs, f\"{ROOT_DIR}/{name}_{SET}_{list(MODELS.values())[0].replace('/', '-')}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': \"'s pretty linear and only makeup-deep \", 'label': 0, 'idx': 60200}\n",
      "['[CLS]', \"'\", 's', 'pretty', 'linear', 'and', 'only', 'makeup', '-', 'deep', '[SEP]']\n",
      "[0.8354315  0.16485864 0.1310181  0.4056108  0.7041902  0.14505213\n",
      " 0.29287553 1.         0.13067044 0.39498818 0.17352831]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from seaborn import heatmap\n",
    "idx = 60200\n",
    "print(dataset[SET][idx])\n",
    "print(globencs[\"tokens\"][idx])\n",
    "print(globencs[\"globenc\"][idx][0] / globencs[\"globenc\"][idx][0].max())\n",
    "heatmap(globencs[\"globenc\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aafc14c9697448b86cbc721816cf1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GlobEnc:   0%|          | 0/2807 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4160127/3142391982.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             logits, norms = model(**input_batch, output_attentions=False, output_norms=False, \n\u001b[1;32m     14\u001b[0m                                                     return_dict=False, output_globenc=True, output_hidden_states=False)\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mglobenc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mbatch_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0midxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "it = iter(dataloader)\n",
    "\n",
    "idxes = []\n",
    "shuffled_globencs, shuffled_cls = [], []\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(steps), desc=\"GlobEnc\"):\n",
    "        batch = next(it)\n",
    "        input_batch = {k: batch[k].to(DEVICE) for k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]}\n",
    "        if SAVE_CLS:\n",
    "            logits, hidden_states, norms = model(**input_batch, output_attentions=False, output_norms=False, \n",
    "                                                    return_dict=False, output_globenc=True, output_hidden_states=True)\n",
    "        else:\n",
    "            logits, norms = model(**input_batch, output_attentions=False, output_norms=False, \n",
    "                                                    return_dict=False, output_globenc=True, output_hidden_states=False)\n",
    "        globenc = norms.squeeze().cpu().numpy()\n",
    "        batch_lengths = batch[\"length\"].numpy()\n",
    "        idxes.extend(batch['idx'].tolist())\n",
    "        shuffled_globencs.extend([globenc[j][:batch_lengths[j],:batch_lengths[j]] for j in range(len(globenc))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /home/modaresi/projects/globenc_analysis/outputs/globencs_elementwise/sst2-e5_validation_bert-base-uncased.pickle\n"
     ]
    }
   ],
   "source": [
    "save_pickle(globencs, f\"{ROOT_DIR}/sst2-e5_validation_bert-base-uncased.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad96aab5654ccd4e66bf13bd728d9d4512a27ebcee40fc973a7275c9c55ebd75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
