{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d1a8d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 20:02:46.869988: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-20 20:02:47.038772: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-20 20:02:47.625224: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/modaresi/.conda/envs/globenc-venv/lib/\n",
      "2023-01-20 20:02:47.625315: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/modaresi/.conda/envs/globenc-venv/lib/\n",
      "2023-01-20 20:02:47.625321: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from copy import deepcopy\n",
    "\n",
    "from GlobEnc.src.modeling.modeling_bert_saliency import BertForSequenceClassification\n",
    "from GlobEnc.src.modeling.modeling_roberta_saliency import RobertaForSequenceClassification\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d312334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"/home/modaresi/projects/globenc_analysis/outputs/models\"\n",
    "outputs_dir = \"/home/modaresi/projects/globenc_analysis/outputs/integrated_saliencies\"\n",
    "\n",
    "# configs = {\n",
    "#     \"mnli\": {\n",
    "#         \"model_path\": lambda step: f\"{models_dir}/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-{step}/\",\n",
    "#         \"output_file_path\": lambda step, agg_t, bl_t: f\"{outputs_dir}/mnli_bert-base-uncased_0001_SEED0042_checkpoint-{step}-{agg_t}-{bl_t}.npy\",\n",
    "#         \"hf_ds\": \"mnli\",\n",
    "#     },\n",
    "#     \"sst2\": {\n",
    "#         \"model_path\": lambda step: f\"{models_dir}/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-{step}/\",\n",
    "#         \"output_file_path\": lambda step, agg_t, bl_t: f\"{outputs_dir}/sst2_bert-base-uncased_0001_SEED0042_checkpoint-{step}-{agg_t}-{bl_t}.npy\",\n",
    "#         \"hf_ds\": \"sst2\",\n",
    "#     }\n",
    "# }\n",
    "\n",
    "MODEL_DATASET_SET_PARTS = [\n",
    "#     (f\"{models_dir}/output_cola_bert-base-uncased_0001_SEED0042/checkpoint-1340\", \"cola\", \"validation\"),\n",
    "#     (f\"{models_dir}/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-10525\", \"sst2\", \"train\", [\"sentence\"]),\n",
    "#     (f\"{models_dir}/output_hatexplain_bert-base-uncased_0001_SEED0042/checkpoint-2405\", \"hatexplain\", \"train\", [\"text\"]),\n",
    "#     (f\"{models_dir}/output_sst2_bert-large-uncased_0001_SEED0042/checkpoint-10525\", \"sst2\", \"validation\"),\n",
    "#     (f\"{models_dir}/output_mrpc_bert-base-uncased_0001_SEED0042/checkpoint-575\", \"mrpc\", \"validation\"),\n",
    "#     (f\"{models_dir}/output_qnli_bert-base-uncased_0001_SEED0042/checkpoint-16370\", \"qnli\", \"train\", [\"question\", \"sentence\"]),\n",
    "#     (f\"{models_dir}/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-61360\", \"mnli\", \"train\", [\"premise\", \"hypothesis\"]),\n",
    "    (f\"WillHeld/roberta-base-sst2\", \"sst2\", \"validation\", [\"sentence\"]),\n",
    "    (f\"WillHeld/roberta-base-mnli\", \"mnli\", \"validation_matched\", [\"premise\", \"hypothesis\"]),\n",
    "]\n",
    "\n",
    "# CONFIG_NAME = \"mnli\"\n",
    "# CONFIG = configs[CONFIG_NAME]\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "# DATA_SECTION = \"validation_matched\"\n",
    "# BASELINE_TYPES = [\"ALL_PAD\", \"ALL_ZERO\"]\n",
    "BASELINE_TYPES = [\"ALL_ZERO\"]\n",
    "AGGREGATION_TYPES = [\"NORM\", \"SUM\"]\n",
    "# AGGREGATION_TYPES = [\"NORM\"]\n",
    "ALPHA_STEPS = 11\n",
    "ALPHAS = np.linspace(0, 1, ALPHA_STEPS)\n",
    "# STEP = 10525\n",
    "# STEP = 61360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e49754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_ds = datasets.load_dataset(\"glue\", CONFIG[\"hf_ds\"])[DATA_SECTION]\n",
    "# original_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6baa5433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"bert-base-uncased\",\n",
    "#     use_fast=True,\n",
    "# )\n",
    "\n",
    "def _get_preprocessing_function(\n",
    "    sentence1_key: str, \n",
    "    sentence2_key: str = None, \n",
    "    label_to_id: dict = None):\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize the texts\n",
    "        args = (\n",
    "            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "        )\n",
    "        result = tokenizer(*args, padding=False, max_length=MAX_LENGTH, truncation=True)\n",
    "\n",
    "        # Map labels to IDs (not necessary for GLUE tasks)\n",
    "        if label_to_id is not None and \"label\" in examples:\n",
    "            result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "        return result\n",
    "    \n",
    "    return preprocess_function\n",
    "\n",
    "# def hatexplain_preprocess_function(examples):\n",
    "#     def mode(lst):\n",
    "#         return(max(set(lst), key=lst.count))\n",
    "#     examples[\"text\"] = \" \".join(str(v) for v in examples[\"post_tokens\"])\n",
    "# #     print(examples[\"annotators\"])\n",
    "#     labels = [mode(val[\"label\"]) for val in examples[\"annotators\"]]\n",
    "#     args = (\n",
    "#         (examples[\"text\"],)\n",
    "#     )\n",
    "#     result = tokenizer(*args, padding=False, max_length=MAX_LENGTH, truncation=True)\n",
    "# #     print(result)\n",
    "# #     result = dict(result)\n",
    "# #     print(\"here\")\n",
    "# #     print()\n",
    "#     print(result)\n",
    "#     result[\"label\"] = labels\n",
    "    \n",
    "#     return result\n",
    "\n",
    "def aggregate_hatexplain(example):\n",
    "    def mode(lst):\n",
    "        return max(set(lst), key=lst.count)\n",
    "    example[\"label\"] = mode(example[\"annotators\"][\"label\"])\n",
    "    example[\"text\"] = \" \".join(example[\"post_tokens\"])\n",
    "    return example\n",
    "    \n",
    "\n",
    "# preprocess_function = _get_preprocessing_function(sentence1_key=\"premise\", sentence2_key=\"hypothesis\")\n",
    "# preprocess_function = _get_preprocessing_function(sentence1_key=\"sentence\")\n",
    "# train_ds = original_ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea96f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if BASELINE_TYPE == \"ALL_PAD\":\n",
    "#     train_ds_base_line = train_ds.to_dict()\n",
    "#     for idx, val in enumerate(train_ds_base_line['input_ids']):\n",
    "#         input_ids = [0 for i in range(len(val))]\n",
    "# #         SEP_tokens = np.where(np.array(row['input_ids'])==102)[0]\n",
    "#         input_ids[0] = 101\n",
    "#         input_ids[-1] = 102\n",
    "# #         input_ids[SEP_tokens] = 102\n",
    "#         train_ds_base_line['input_ids'][idx] = input_ids\n",
    "        \n",
    "#     train_ds_base_line = datasets.Dataset.from_dict(train_ds_base_line)\n",
    "    \n",
    "#     train_ds_base_line.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "#     collator = DataCollatorWithPadding(tokenizer, True, MAX_LENGTH, return_tensors=\"pt\")\n",
    "#     baseline_dataloader = torch.utils.data.DataLoader(train_ds_base_line, batch_size=BATCH_SIZE, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132895cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "# collator = DataCollatorWithPadding(tokenizer, True, MAX_LENGTH, return_tensors=\"pt\")\n",
    "# dataloader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "# dataset_size = len(train_ds)\n",
    "# steps = int(np.ceil(dataset_size / BATCH_SIZE))\n",
    "# num_labels = len(set(original_ds['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e405ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained(CONFIG[\"model_path\"](STEP))\n",
    "# model.to(torch.device(\"cuda:0\"))\n",
    "# model.eval()\n",
    "\n",
    "# # all_sals = torch.zeros(size=(dataset_size, MAX_LENGTH)).cuda()\n",
    "# all_sals = np.zeros(shape=(dataset_size, MAX_LENGTH))\n",
    "# it = iter(dataloader)\n",
    "# if BASELINE_TYPE == \"ALL_PAD\":\n",
    "#     baseline_it = iter(baseline_dataloader)\n",
    "\n",
    "# for i in tqdm(range(steps)):\n",
    "#     batch = next(it)\n",
    "#     batch = {k: v.to(torch.device('cuda:0')) for k, v in batch.items()}\n",
    "#     labels = batch['labels']\n",
    "#     output = model(**batch, output_hidden_states=True)\n",
    "#     x_reps = output.hidden_states[0]\n",
    "#     output.hidden_states[0].retain_grad()\n",
    "#     logits = output.logits\n",
    "#     target_class_l_sum = torch.gather(logits, 1, labels.unsqueeze(-1)).sum()\n",
    "#     target_class_l_sum.backward()\n",
    "    \n",
    "#     gradients = output.hidden_states[0].grad\n",
    "#     model.zero_grad()\n",
    "    \n",
    "#     if BASELINE_TYPE == \"ALL_PAD\":\n",
    "#         baseline_batch = next(baseline_it)\n",
    "#         baseline_batch = {k: v.to(torch.device('cuda:0')) for k, v in baseline_batch.items()}\n",
    "#         labels = batch['labels']\n",
    "#         baseline_output = model(**baseline_batch, output_hidden_states=True)\n",
    "#         x_baseline_reps = baseline_output.hidden_states[0]\n",
    "#         baseline_output.hidden_states[0].retain_grad()\n",
    "#         baseline_logits = baseline_output.logits\n",
    "#         target_class_l_sum = torch.gather(baseline_logits, 1, labels.unsqueeze(-1)).sum()\n",
    "#         target_class_l_sum.backward()\n",
    "        \n",
    "#         gradients += baseline_output.hidden_states[0].grad\n",
    "#         model.zero_grad()\n",
    "        \n",
    "#     for idx, alpha in enumerate(ALPHAS):\n",
    "#         if alpha == 0.0 or alpha == 1.0:\n",
    "#             continue\n",
    "        \n",
    "        \n",
    "#         if BASELINE_TYPE == \"ALL_PAD\":\n",
    "#             x_alpha = x_baseline_reps + alpha * (x_reps - x_baseline_reps)\n",
    "#         elif BASELINE_TYPE == \"ALL_ZERO\":\n",
    "#             x_alpha = alpha * x_reps\n",
    "        \n",
    "#         inputs = x_alpha.detach()\n",
    "#         inputs.requires_grad = True\n",
    "#         labels = batch['labels']\n",
    "#         output = model(inputs_embeds=inputs, labels=labels, output_hidden_states=True)\n",
    "#         output.hidden_states[0].retain_grad()\n",
    "#         logits = output.logits\n",
    "#         target_class_l_sum = torch.gather(logits, 1, labels.unsqueeze(-1)).sum()\n",
    "#         target_class_l_sum.backward()\n",
    "        \n",
    "#         gradients += output.hidden_states[0].grad\n",
    "        \n",
    "#         model.zero_grad()\n",
    "        \n",
    "#     if BASELINE_TYPE == \"ALL_PAD\":\n",
    "#         integrated_saliency =  gradients * (x_reps - x_baseline_reps)\n",
    "#     elif BASELINE_TYPE == \"ALL_ZERO\":\n",
    "#         integrated_saliency =  gradients * x_reps\n",
    "        \n",
    "#     if AGGREGATION_TYPE == \"SUM\":\n",
    "#         saliencies = torch.sum(integrated_saliency, dim=-1).detach().cpu()\n",
    "#     elif AGGREGATION_TYPE == \"NORM\":\n",
    "#         saliencies = torch.norm(integrated_saliency, dim=-1).detach().cpu()\n",
    "    \n",
    "#     length = saliencies.size()[1]\n",
    "#     all_sals[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :length] = saliencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e8d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(CONFIG[\"output_file_path\"](STEP, AGGREGATION_TYPE, BASELINE_TYPE), all_sals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7de582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac41c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_integrated_saliencies(dataloader, baseline_dataloader, baseline_type, steps, model, aggregation_type,\n",
    "                             prediction_based=True):\n",
    "    all_sals_norm_aggregated = np.zeros(shape=(dataset_size, MAX_LENGTH))\n",
    "    all_sals_sum_aggregated = np.zeros(shape=(dataset_size, MAX_LENGTH))\n",
    "    it = iter(dataloader)\n",
    "    if baseline_type == \"ALL_PAD\":\n",
    "        baseline_it = iter(baseline_dataloader)\n",
    "\n",
    "    for i in tqdm(range(steps)):\n",
    "        batch = next(it)\n",
    "        batch = {k: v.to(torch.device('cuda:0')) for k, v in batch.items()}\n",
    "        labels = batch['labels']\n",
    "        output = model(**batch, output_hidden_states=True)\n",
    "        x_reps = output.hidden_states[0]\n",
    "        output.hidden_states[0].retain_grad()\n",
    "        logits = output.logits\n",
    "        if prediction_based:\n",
    "            target_class_l_sum = torch.gather(logits, 1, torch.argmax(logits, dim=-1).unsqueeze(-1)).sum()\n",
    "        else:\n",
    "            target_class_l_sum = torch.gather(logits, 1, labels.unsqueeze(-1)).sum()\n",
    "        target_class_l_sum.backward()\n",
    "\n",
    "        gradients = output.hidden_states[0].grad\n",
    "        predictions = torch.zeros(len(logits)).cuda()\n",
    "        for idx, val in enumerate(logits):\n",
    "            predictions[idx] = torch.argmax(val)\n",
    "#         predictions = torch.argmax(logits.detach(), dim=-1).detach()\n",
    "        predictions = predictions.type(torch.int64)\n",
    "        model.zero_grad()\n",
    "\n",
    "        if baseline_type == \"ALL_PAD\":\n",
    "            baseline_batch = next(baseline_it)\n",
    "            baseline_batch = {k: v.to(torch.device('cuda:0')) for k, v in baseline_batch.items()}\n",
    "            labels = batch['labels']\n",
    "            baseline_output = model(**baseline_batch, output_hidden_states=True)\n",
    "            x_baseline_reps = baseline_output.hidden_states[0]\n",
    "            baseline_output.hidden_states[0].retain_grad()\n",
    "            baseline_logits = baseline_output.logits\n",
    "            if prediction_based:\n",
    "#                 target_class_l_sum = torch.gather(baseline_logits, 1, torch.argmax(baseline_logits, dim=-1).unsqueeze(-1)).sum()\n",
    "                target_class_l_sum = torch.gather(baseline_logits, 1, predictions.unsqueeze(-1)).sum()\n",
    "            else:\n",
    "                target_class_l_sum = torch.gather(baseline_logits, 1, labels.unsqueeze(-1)).sum()\n",
    "            target_class_l_sum.backward()\n",
    "\n",
    "            gradients += baseline_output.hidden_states[0].grad\n",
    "            model.zero_grad()\n",
    "\n",
    "        for idx, alpha in enumerate(ALPHAS):\n",
    "            if alpha == 0.0 or alpha == 1.0:\n",
    "                continue\n",
    "\n",
    "\n",
    "            if baseline_type == \"ALL_PAD\":\n",
    "                x_alpha = x_baseline_reps + alpha * (x_reps - x_baseline_reps)\n",
    "            elif baseline_type == \"ALL_ZERO\":\n",
    "                x_alpha = alpha * x_reps\n",
    "\n",
    "            inputs = x_alpha.detach()\n",
    "            inputs.requires_grad = True\n",
    "            labels = batch['labels']\n",
    "            output = model(inputs_embeds=inputs, labels=labels, output_hidden_states=True)\n",
    "            output.hidden_states[0].retain_grad()\n",
    "            logits = output.logits\n",
    "            if prediction_based:\n",
    "#                 target_class_l_sum = torch.gather(logits, 1, torch.argmax(logits, dim=-1).unsqueeze(-1)).sum()\n",
    "                target_class_l_sum = torch.gather(logits, 1, predictions.unsqueeze(-1)).sum()\n",
    "            else:\n",
    "                target_class_l_sum = torch.gather(logits, 1, labels.unsqueeze(-1)).sum()\n",
    "            target_class_l_sum.backward()\n",
    "\n",
    "            gradients += output.hidden_states[0].grad\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "        if baseline_type == \"ALL_PAD\":\n",
    "            integrated_saliency =  gradients * (x_reps - x_baseline_reps)\n",
    "        elif baseline_type == \"ALL_ZERO\":\n",
    "            integrated_saliency =  gradients * x_reps\n",
    "\n",
    "#         if aggregation_type == \"SUM\":\n",
    "#             saliencies = torch.sum(integrated_saliency, dim=-1).detach().cpu()\n",
    "#         elif aggregation_type == \"NORM\":\n",
    "#             saliencies = torch.norm(integrated_saliency, dim=-1).detach().cpu()\n",
    "        saliencies_norm_aggregated = torch.norm(integrated_saliency, dim=-1).detach().cpu()\n",
    "        saliencies_sum_aggregated = torch.sum(integrated_saliency, dim=-1).detach().cpu()\n",
    "\n",
    "        length = saliencies_norm_aggregated.size()[1]\n",
    "        all_sals_norm_aggregated[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :length] = saliencies_norm_aggregated\n",
    "        all_sals_sum_aggregated[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :length] = saliencies_sum_aggregated\n",
    "    \n",
    "    return all_sals_norm_aggregated, all_sals_sum_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e863b637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /opt/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad (last modified on Sat Sep  3 17:51:06 2022) since it couldn't be found locally at glue., or remotely on the Hugging Face Hub.\n",
      "Reusing dataset glue (/opt/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f3354896c444d28a97aad1bb2ce640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function _get_preprocessing_function.<locals>.preprocess_function at 0x7f7f0f2cd710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a94af6f1e844986b36bff9a810a9152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a3faf38a1848ee97bcb21bb136d61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/modaresi/.conda/envs/globenc-venv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2280: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "Reusing dataset glue (/opt/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234cc98c8339455d9646804dd527bc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ac7b3dfeb4450e9e38279d306e3896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199cec6fee514c5bab55bffeb33c9099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_reports = dict()\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"bert-base-uncased\",\n",
    "#     use_fast=True,\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "for model_dataset_set_parts in MODEL_DATASET_SET_PARTS:\n",
    "    model_checkpoint, task_name, set_of_data, sample_parts = model_dataset_set_parts\n",
    "#     model = BertForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "    model.to(torch.device(\"cuda:0\"))\n",
    "    model.eval()\n",
    "    \n",
    "    if task_name == \"hatexplain\":\n",
    "        original_ds = datasets.load_dataset(task_name)[set_of_data].map(aggregate_hatexplain)\n",
    "        preprocess_function = _get_preprocessing_function(sentence1_key=sample_parts[0])\n",
    "        train_ds = original_ds.map(preprocess_function, batched=True)\n",
    "    else:\n",
    "        original_ds = datasets.load_dataset(\"glue\", task_name)[set_of_data]\n",
    "        if len(sample_parts) == 1:\n",
    "            preprocess_function = _get_preprocessing_function(sentence1_key=sample_parts[0])\n",
    "        elif len(sample_parts) == 2:\n",
    "            preprocess_function = _get_preprocessing_function(sentence1_key=sample_parts[0], sentence2_key=sample_parts[1])\n",
    "        train_ds = original_ds.map(preprocess_function, batched=True)\n",
    "    \n",
    "#     train_ds.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "    train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    collator = DataCollatorWithPadding(tokenizer, True, MAX_LENGTH, return_tensors=\"pt\")\n",
    "    dataloader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "    dataset_size = len(train_ds)\n",
    "    steps = int(np.ceil(dataset_size / BATCH_SIZE))\n",
    "    num_labels = len(set(original_ds['label']))\n",
    "    \n",
    "    for bl_t in BASELINE_TYPES:\n",
    "#         for agg_t in AGGREGATION_TYPES:\n",
    "#             for gradient_base in [\"prediction_based\", \"label_based\"]:\n",
    "        for gradient_base in [\"prediction_based\"]:\n",
    "            t1 = time.time()\n",
    "            if bl_t == \"ALL_PAD\":\n",
    "                train_ds_base_line = train_ds.to_dict()\n",
    "                for idx, val in enumerate(train_ds_base_line['input_ids']):\n",
    "                    input_ids = [0 for i in range(len(val))]\n",
    "                    input_ids[0] = 101\n",
    "                    input_ids[-1] = 102\n",
    "                    train_ds_base_line['input_ids'][idx] = input_ids\n",
    "\n",
    "                train_ds_base_line = datasets.Dataset.from_dict(train_ds_base_line)\n",
    "\n",
    "                train_ds_base_line.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "                collator = DataCollatorWithPadding(tokenizer, True, MAX_LENGTH, return_tensors=\"pt\")\n",
    "                baseline_dataloader = torch.utils.data.DataLoader(train_ds_base_line, batch_size=BATCH_SIZE, collate_fn=collator)\n",
    "\n",
    "            t2 = time.time()\n",
    "            saliencies_norm_aggregated, saliencies_sum_aggregated = get_integrated_saliencies(\n",
    "                dataloader=dataloader,\n",
    "                baseline_dataloader=baseline_dataloader if bl_t == \"ALL_PAD\" else None,\n",
    "                baseline_type=bl_t,\n",
    "                steps=steps,\n",
    "                model=model,\n",
    "                aggregation_type=AGGREGATION_TYPES[0],\n",
    "                prediction_based=(gradient_base==\"prediction_based\")\n",
    "            )\n",
    "            t3 = time.time()\n",
    "\n",
    "            time_reports[f\"{task_name}-{set_of_data}-{bl_t}-{AGGREGATION_TYPES[0]}-{gradient_base}\"] = (t3 - t1, t3 - t2)\n",
    "\n",
    "#             file_name_norm_aggregated = f\"[{task_name}]_[{set_of_data}]_[{'-'.join(model_checkpoint.split('/')[-2:])}]_[IG_{bl_t}_{AGGREGATION_TYPES[0]}_{gradient_base}]\"\n",
    "#             file_name_sum_aggregated = f\"[{task_name}]_[{set_of_data}]_[{'-'.join(model_checkpoint.split('/')[-2:])}]_[IG_{bl_t}_{AGGREGATION_TYPES[1]}_{gradient_base}]\"\n",
    "            file_name_norm_aggregated = f\"[{task_name}]_[{set_of_data}]_[{model_checkpoint.split('/')[-1]}]_[IG_{bl_t}_{AGGREGATION_TYPES[0]}_{gradient_base}]\"\n",
    "            file_name_sum_aggregated = f\"[{task_name}]_[{set_of_data}]_[{model_checkpoint.split('/')[-1]}]_[IG_{bl_t}_{AGGREGATION_TYPES[1]}_{gradient_base}]\"\n",
    "                      \n",
    "            np.save(f\"{outputs_dir}/{file_name_norm_aggregated}.npy\", saliencies_norm_aggregated)\n",
    "            np.save(f\"{outputs_dir}/{file_name_sum_aggregated}.npy\", saliencies_sum_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4da625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sst2-validation-ALL_ZERO-NORM-prediction_based': (215.85352873802185, 215.85351538658142), 'qnli-validation-ALL_ZERO-NORM-prediction_based': (2784.8672778606415, 2784.867276906967), 'mnli-validation_matched-ALL_ZERO-NORM-prediction_based': (4357.458532810211, 4357.458531618118), 'hatexplain-validation-ALL_ZERO-NORM-prediction_based': (625.162024974823, 625.1620240211487)}\n"
     ]
    }
   ],
   "source": [
    "print(time_reports)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad96aab5654ccd4e66bf13bd728d9d4512a27ebcee40fc973a7275c9c55ebd75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
