{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 10:49:38.120146: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-29 10:49:38.253376: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-29 10:49:38.855633: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/modaresi/.conda/envs/globenc-venv/lib/\n",
      "2022-11-29 10:49:38.855716: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/modaresi/.conda/envs/globenc-venv/lib/\n",
      "2022-11-29 10:49:38.855722: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 10:49:39.613647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 10:49:39.620550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 10:49:39.620701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(\n",
    "        gpus[int(0)],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=15000)])\n",
    "        tf.config.experimental.set_visible_devices(gpus[int(0)], 'GPU')\n",
    "        # tf.config.experimental.set_memory_growth(gpus[int(SELECTED_GPU)], True)\n",
    "        # tf.config.experimental.se.set_per_process_memory_fraction(0.92)\n",
    "        print(gpus[int(0)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('GPU not found!')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFBertForSequenceClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"/home/modaresi/projects/globenc_analysis/outputs/models\"\n",
    "outputs_dir = \"/home/modaresi/projects/globenc_analysis/outputs/HTAs\"\n",
    "DATA_SECTION = \"validation\"\n",
    "configs = {\n",
    "    \"mnli\": {\n",
    "        \"model_path\": lambda epoch: f\"{models_dir}/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-{12272*epoch}/\",\n",
    "        \"output_file_path\": lambda epoch: f\"{outputs_dir}/mnli_{DATA_SECTION}_bert-base-uncased_0001_SEED0042_checkpoint-{12272*epoch}.pkl\",\n",
    "        \"hf_ds\": \"mnli\",\n",
    "    },\n",
    "    \"sst2\": {\n",
    "        \"model_path\": lambda epoch: f\"{models_dir}/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-{epoch*2105}/\",\n",
    "        \"output_file_path\": lambda epoch: f\"{outputs_dir}/sst2_{DATA_SECTION}_bert-base-uncased_0001_SEED0042_checkpoint-{epoch*2105}.pkl\",\n",
    "        \"hf_ds\": \"sst2\",\n",
    "    }\n",
    "}\n",
    "\n",
    "TASK = \"sst2\"\n",
    "CONFIG = configs[TASK]\n",
    "MAX_LENGTH = 128\n",
    "SEED = 42\n",
    "EPOCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/opt/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90acb1159614d9380b83a30bddc70eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
    "\n",
    "MAX_LENGTH = 64\n",
    "\n",
    "actual_task = \"mnli\" if TASK == \"mnli-mm\" else TASK\n",
    "dataset = datasets.load_dataset(\"glue\", actual_task)\n",
    "# metric = datasets.load_metric('glue', actual_task)\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "SENTENCE1_KEY, SENTENCE2_KEY = task_to_keys[TASK]\n",
    "tokenizer = None\n",
    "sel_dataset = None\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pathlib\n",
    "def preprocess_function_wrapped(tokenizer):\n",
    "    def preprocess_function(examples):\n",
    "        # Tokenize the texts\n",
    "        args = (\n",
    "            (examples[SENTENCE1_KEY],) if SENTENCE2_KEY is None else (examples[SENTENCE1_KEY], examples[SENTENCE2_KEY])\n",
    "        )\n",
    "        result = tokenizer(*args, padding=False, max_length=MAX_LENGTH, truncation=True)\n",
    "        return result\n",
    "    return preprocess_function\n",
    "\n",
    "def token_id_to_tokens_mapper(tokenizer, sample):\n",
    "    length = len(sample[\"input_ids\"])\n",
    "    return tokenizer.convert_ids_to_tokens(sample[\"input_ids\"])[:length], length\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    pathlib.Path(os.path.dirname(path)).mkdir(parents=True, exist_ok=True) \n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"Saved {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 10:50:03.302363: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-29 10:50:03.308628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 10:50:03.309094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 10:50:03.309432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 10:50:04.862247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 10:50:04.862451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 10:50:04.862571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-29 10:50:04.862681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15000 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:03:00.0, compute capability: 7.5\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Parameter 'function'=<function preprocess_function_wrapped.<locals>.preprocess_function at 0x7fcf7ff779e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9a01853e1b4b0bb075f664b6c636ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = CONFIG[\"model_path\"](EPOCH)\n",
    "model = TFBertForSequenceClassification.from_pretrained(path, from_pt=True, output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, use_fast=True, max_length=128) if tokenizer is None else tokenizer\n",
    "sel_dataset = dataset[DATA_SECTION].map(preprocess_function_wrapped(tokenizer), batched=True, batch_size=1024) if sel_dataset is None else sel_dataset\n",
    "dataset_size = len(sel_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19fa9ed89924ba48b36f3d913d4b15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/872 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /home/modaresi/projects/globenc_analysis/outputs/HTAs/sst2_validation_bert-base-uncased_0001_SEED0042_checkpoint-10525.pkl\n"
     ]
    }
   ],
   "source": [
    "HTAs = []\n",
    "HTAs_angular = []\n",
    "\n",
    "# @tf.function(input_signature=[{'input_ids': tf.TensorSpec(shape=[1, None], dtype=tf.int32),\n",
    "#             'token_type_ids': tf.TensorSpec(shape=[1, None], dtype=tf.int32),\n",
    "#             'attention_mask': tf.TensorSpec(shape=[1, None], dtype=tf.int32)}, \n",
    "#             tf.TensorSpec(shape=(1,), dtype=tf.int32)])\n",
    "@tf.function(input_signature=[{'input_ids': tf.TensorSpec(shape=[1, None], dtype=tf.int32),\n",
    "            'token_type_ids': tf.TensorSpec(shape=[1, None], dtype=tf.int32),\n",
    "            'attention_mask': tf.TensorSpec(shape=[1, None], dtype=tf.int32)}])\n",
    "def get_grads(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(inputs)\n",
    "        input_embeds = outputs.hidden_states[0]\n",
    "        # out_embeds = tf.gather(outputs.hidden_states[-1], i, batch_dims=1)\n",
    "        out_embeds = outputs.hidden_states[-1][:, 0]\n",
    "\n",
    "    grads = tape.batch_jacobian(out_embeds, input_embeds)\n",
    "    inputXgradients = grads * tf.expand_dims(input_embeds, axis=1)\n",
    "    return tf.linalg.norm(inputXgradients, axis=(1,3)), tf.reduce_sum(inputXgradients, axis=(1,3))\n",
    "\n",
    "for idx in tqdm(range(dataset_size)):\n",
    "    length = len(sel_dataset[idx]['input_ids'])\n",
    "    hta = np.zeros((length), dtype=np.float32)\n",
    "    hta_angular = np.zeros((length), dtype=np.float32)\n",
    "    feats = {\n",
    "            'input_ids': tf.constant([sel_dataset[idx]['input_ids']], dtype=tf.int32),\n",
    "            'attention_mask': tf.constant([sel_dataset[idx]['attention_mask']], dtype=tf.int32),\n",
    "            'token_type_ids': tf.constant([sel_dataset[idx]['token_type_ids']], dtype=tf.int32),\n",
    "    }\n",
    "    i = 0\n",
    "    norms, angleDot = get_grads(feats) \n",
    "    # hta[i, :length] = np.linalg.norm(grads, axis=(1,3))\n",
    "    # hta_angular[i, :length] = np.sum(grads, axis=(1,3))\n",
    "    HTAs.append(norms.numpy())\n",
    "    HTAs_angular.append(angleDot.numpy())\n",
    "\n",
    "save_pickle({\"HTAs\": HTAs, \"HTAs_angular\": HTAs_angular}, CONFIG[\"output_file_path\"](EPOCH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad96aab5654ccd4e66bf13bd728d9d4512a27ebcee40fc973a7275c9c55ebd75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
