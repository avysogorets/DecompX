{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install datasets==1.18.3\n",
    "# ! pip install transformers==4.18.0\n",
    "! git clone https://github.com/mohsenfayyaz/GlobEnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import datasets\n",
    "import pickle\n",
    "import pathlib\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from GlobEnc.src.modeling.modeling_bert import BertForSequenceClassification\n",
    "from GlobEnc.src.modeling.modeling_electra import ElectraForSequenceClassification\n",
    "from GlobEnc.src.attention_rollout import AttentionRollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"./outputs_globenc\"\n",
    "# MODELS = {\n",
    "#     \"sst2-e0\": \"bert-base-uncased\",\n",
    "#     \"sst2-e1\": \"/home/modaresi/globenc_extension/outputs/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-2105\",\n",
    "#     \"sst2-e2\": \"/home/modaresi/globenc_extension/outputs/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-4210\",\n",
    "#     \"sst2-e3\": \"/home/modaresi/globenc_extension/outputs/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-6315\",\n",
    "#     \"sst2-e4\": \"/home/modaresi/globenc_extension/outputs/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-8420\",\n",
    "#     \"sst2-e5\": \"/home/modaresi/globenc_extension/outputs/output_sst2_bert-base-uncased_0001_SEED0042/checkpoint-10525\",\n",
    "# }\n",
    "MODELS = {\n",
    "    \"mnli-e0\": \"bert-base-uncased\",\n",
    "    \"mnli-e1\": \"/home/modaresi/globenc_extension/outputs/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-12272\",\n",
    "    \"mnli-e2\": \"/home/modaresi/globenc_extension/outputs/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-24544\",\n",
    "    \"mnli-e3\": \"/home/modaresi/globenc_extension/outputs/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-36816\",\n",
    "    \"mnli-e4\": \"/home/modaresi/globenc_extension/outputs/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-49088\",\n",
    "    \"mnli-e5\": \"/home/modaresi/globenc_extension/outputs/output_mnli_bert-base-uncased_0001_SEED0042/checkpoint-61360\",\n",
    "}\n",
    "\n",
    "TASK = \"mnli\"  # sst2/mnli\n",
    "SET = \"train\"  # train/validation/validation_matched\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "actual_task = \"mnli\" if TASK == \"mnli-mm\" else TASK\n",
    "dataset = datasets.load_dataset(\"glue\", actual_task)\n",
    "metric = datasets.load_metric('glue', actual_task)\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "SENTENCE1_KEY, SENTENCE2_KEY = task_to_keys[TASK]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_globenc(model, tokenizer, data):\n",
    "    if SENTENCE2_KEY is None:\n",
    "        tokenized_sentence = tokenizer.encode_plus(data[SENTENCE1_KEY], return_tensors=\"pt\")\n",
    "    else:\n",
    "        tokenized_sentence = tokenizer.encode_plus(data[SENTENCE1_KEY], data[SENTENCE2_KEY], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        tokenized_sentence = tokenized_sentence.to(DEVICE)\n",
    "        model.to(DEVICE)\n",
    "        logits, norms = model(**tokenized_sentence, output_attentions=False, output_norms=True, return_dict=False)\n",
    "    num_layers = 12\n",
    "    norm_nenc = torch.stack([norms[i][4] for i in range(num_layers)]).squeeze().cpu().numpy()\n",
    "    globenc = AttentionRollout().compute_flows([norm_nenc], output_hidden_states=True, disable_tqdm=True)[0]\n",
    "    globenc = np.array(globenc)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_sentence[\"input_ids\"][0])\n",
    "    return globenc, tokens\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    pathlib.Path(os.path.dirname(path)).mkdir(parents=True, exist_ok=True) \n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"Saved {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, path in tqdm(MODELS.items(), desc=\"Models\"):\n",
    "    model = BertForSequenceClassification.from_pretrained(path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "    globencs = {\"globenc\": [], \"tokens\": []}\n",
    "    for data in tqdm(dataset[SET], total=len(dataset[SET])):\n",
    "        globenc, tokens = extract_globenc(model, tokenizer, data)\n",
    "        globencs[\"globenc\"].append(globenc)\n",
    "        globencs[\"tokens\"].append(tokens)\n",
    "    save_pickle(globencs, f\"{ROOT_DIR}/{name}_{SET}.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched GlobEnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_globenc_batch(model, tokenizer, batch):\n",
    "    if SENTENCE2_KEY is None:\n",
    "        tokenized_sentences = tokenizer(batch[SENTENCE1_KEY], return_tensors=\"pt\", padding=True)\n",
    "    else:\n",
    "        tokenized_sentences = tokenizer(batch[SENTENCE1_KEY], batch[SENTENCE2_KEY], return_tensors=\"pt\", padding=True)\n",
    "    tokenized_sentences = tokenized_sentences.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        model.to(DEVICE)\n",
    "#         logits, norms = model(**tokenized_sentences, output_attentions=False, output_norms=True, return_dict=False)\n",
    "        logits, norms = model(**tokenized_sentences, output_attentions=False, output_norms=False, return_dict=False, output_globenc=True)\n",
    "#     num_layers = 12\n",
    "#     norm_nenc = torch.stack([norms[i][4] for i in range(num_layers)]).squeeze().cpu().numpy()  # (12, batch, 78, 78)\n",
    "    tokenized_len = torch.sum(tokenized_sentences['attention_mask'], dim=-1)\n",
    "#     globencs, tokens = [], []\n",
    "#     for idx in range(norm_nenc.shape[1]):\n",
    "#         norm_nenc_idx = norm_nenc[:, idx, :tokenized_len[idx], :tokenized_len[idx]]\n",
    "#         globenc = AttentionRollout().compute_flows([norm_nenc_idx], output_hidden_states=True, disable_tqdm=True)[0]\n",
    "#         globencs.append(np.array(globenc))\n",
    "#         tokens.append(tokenizer.convert_ids_to_tokens(tokenized_sentences[\"input_ids\"][idx])[:tokenized_len[idx]])\n",
    "    \n",
    "    globenc = norms.squeeze().cpu().numpy()\n",
    "    globencs, tokens = [], []\n",
    "    for idx in range(len(globenc)):\n",
    "        globencs.append(np.array(globenc[idx, :tokenized_len[idx], :tokenized_len[idx]]))\n",
    "        tokens.append(tokenizer.convert_ids_to_tokens(tokenized_sentences[\"input_ids\"][idx])[:tokenized_len[idx]])\n",
    "    return globencs, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for checkpoint_name, path in tqdm(MODELS.items(), desc=\"Models\"):\n",
    "    globencs = {\"globenc\": [], \"tokens\": []}\n",
    "    model = BertForSequenceClassification.from_pretrained(path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path, use_fast=True, max_length=128)\n",
    "    dataloader = DataLoader(dataset[SET], batch_size=4)\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        globenc, tokens = extract_globenc_batch(model, tokenizer, batch)\n",
    "        globencs[\"globenc\"].extend(globenc)\n",
    "        globencs[\"tokens\"].extend(tokens)\n",
    "    save_pickle(globencs, f\"{ROOT_DIR}/{checkpoint_name}_{SET}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
